{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Veolia\n"
      ],
      "metadata": {
        "id": "TEKx6Dp7PJOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwr7IyW9C5hb",
        "outputId": "671fa256-9bb5-4ed2-f1bb-8d76e1924b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "VEOLIA ETL TRANSFORMATION\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING ETL MAPPINGS FOR: Veolia\n",
            "================================================================================\n",
            "✓ Déchet sheet: 103 mappings for 'Veolia'\n",
            "  - 25 explicit mappings\n",
            "  - Default (for empty): None\n",
            "✓ Paramètres sheet: 121 Déchet fin → Déchets agrégé mappings\n",
            "✓ Traitement générique: 169 treatment mappings\n",
            "✓ Site sheet: 97 sites for 'Veolia'\n",
            "\n",
            "================================================================================\n",
            "READING INPUT FILES\n",
            "================================================================================\n",
            "✓ Veolia - Toulouse: 1 rows\n",
            "✓ Veolia - Grasse: 9 rows\n",
            "\n",
            "================================================================================\n",
            "READING TEMPLATE\n",
            "================================================================================\n",
            "✓ Template: 64 columns\n",
            "\n",
            "================================================================================\n",
            "TRANSFORMING DATA\n",
            "================================================================================\n",
            "✓ Transformed 10 rows\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "✓ Saved: /content/Veolia_Registre_Agrege_OUTPUT.xlsx\n",
            "\n",
            "================================================================================\n",
            "✓✓✓ SUCCESS ✓✓✓\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Veolia ETL Transformation Script (Google Colab Version)\n",
        "Transforms Veolia waste registry data to Urbyn aggregated format.\n",
        "\n",
        "Uses ETL mapping file DIRECTLY:\n",
        "- Déchet sheet: Déchets prestataire → Déchet fin\n",
        "- Paramètres sheet: Déchet fin → Déchets agrégé\n",
        "- Traitement générique sheet: Treatment code mapping\n",
        "- Site sheet: Site name mapping\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR ENVIRONMENT\n",
        "# =============================================================================\n",
        "INPUT_FILES = {\n",
        "    'Veolia - Toulouse': '/content/Reporting_Veolia - Toulouse_Capegmini_03_25.xlsx',\n",
        "    'Veolia - Grasse': '/content/Reporting_Veolia - Grasse_Capegmini_03_25.xlsx',\n",
        "}\n",
        "TEMPLATE_FILE = '/content/Modèle vierge de Registre des déchets et Reporting des coûts SLIMAN.xlsx'\n",
        "ETL_FILE = '/content/ETL _ Mapping registre déchets prestataire vers Urbyn.xlsx'\n",
        "OUTPUT_FILE = '/content/Veolia_Registre_Agrege_OUTPUT.xlsx'\n",
        "\n",
        "# Prestataire search pattern (partial match in ETL)\n",
        "PRESTATAIRE_PATTERN = 'Veolia'\n",
        "\n",
        "TEMPLATE_HEADER_ROW = 8\n",
        "OUTPUT_SHEET = 'Registre des déchets (Mouvement'\n",
        "\n",
        "CLIENT = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE = \"Capgemini\"\n",
        "GROUPE_PRESTATAIRE = \"Veolia\"\n",
        "TYPE_PRESTATAIRE = \"Privé\"\n",
        "\n",
        "\n",
        "class ETLMapper:\n",
        "    \"\"\"Loads and applies mappings from the ETL file\"\"\"\n",
        "\n",
        "    def __init__(self, etl_file, prestataire_pattern):\n",
        "        self.etl_file = etl_file\n",
        "        self.prestataire_pattern = prestataire_pattern\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"LOADING ETL MAPPINGS FOR: {prestataire_pattern}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.load_dechet_mapping()\n",
        "        self.load_dechet_to_agrege_mapping()\n",
        "        self.load_traitement_mapping()\n",
        "        self.load_site_mapping()\n",
        "\n",
        "    def load_dechet_mapping(self):\n",
        "        \"\"\"Load Déchet sheet: Déchets prestataire → Déchet fin\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Déchet')\n",
        "\n",
        "        # Filter by prestataire pattern\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(\n",
        "            self.prestataire_pattern, case=False, na=False\n",
        "        )\n",
        "        df_filtered = df[mask]\n",
        "\n",
        "        print(f\"✓ Déchet sheet: {len(df_filtered)} mappings for '{self.prestataire_pattern}'\")\n",
        "\n",
        "        # Build lookup: déchets prestataire → déchet fin\n",
        "        self.dechet_lookup = {}\n",
        "        self.default_dechet_fin = None\n",
        "\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            prest = row.get('Nom des déchets prestataire')\n",
        "            urbyn = row.get('Nom des déchets Urbyn')\n",
        "\n",
        "            if pd.isna(prest):\n",
        "                # Default mapping for empty/NaN waste names\n",
        "                if pd.notna(urbyn):\n",
        "                    self.default_dechet_fin = urbyn\n",
        "            elif pd.notna(urbyn):\n",
        "                self.dechet_lookup[str(prest).strip().lower()] = urbyn\n",
        "\n",
        "        print(f\"  - {len(self.dechet_lookup)} explicit mappings\")\n",
        "        print(f\"  - Default (for empty): {self.default_dechet_fin}\")\n",
        "\n",
        "    def load_dechet_to_agrege_mapping(self):\n",
        "        \"\"\"Load Paramètres sheet: Déchet fin (Name) → Déchets agrégé (Category)\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Paramètres')\n",
        "\n",
        "        self.dechet_to_agrege = {}\n",
        "        for _, row in df[['Category', 'Name']].dropna().iterrows():\n",
        "            name = str(row['Name']).strip().lower()\n",
        "            category = str(row['Category']).strip()\n",
        "            self.dechet_to_agrege[name] = category\n",
        "\n",
        "        print(f\"✓ Paramètres sheet: {len(self.dechet_to_agrege)} Déchet fin → Déchets agrégé mappings\")\n",
        "\n",
        "    def load_traitement_mapping(self):\n",
        "        \"\"\"Load Traitement générique sheet\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Traitement générique')\n",
        "\n",
        "        self.traitement_lookup = {}\n",
        "        for _, row in df.iterrows():\n",
        "            key = str(row.get('Concatener déchet & code de traitement prestataire', '')).strip()\n",
        "            if key:\n",
        "                self.traitement_lookup[key] = {\n",
        "                    'code': row.get('Code traitement retraité'),\n",
        "                    'traitement': row.get('Traitement')\n",
        "                }\n",
        "\n",
        "        print(f\"✓ Traitement générique: {len(self.traitement_lookup)} treatment mappings\")\n",
        "\n",
        "    def load_site_mapping(self):\n",
        "        \"\"\"Load Site sheet\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Site')\n",
        "\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(\n",
        "            self.prestataire_pattern, case=False, na=False\n",
        "        )\n",
        "        df_filtered = df[mask]\n",
        "\n",
        "        print(f\"✓ Site sheet: {len(df_filtered)} sites for '{self.prestataire_pattern}'\")\n",
        "\n",
        "        self.site_lookup = {}\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            site_prest = row.get('Nom site prestataire')\n",
        "            if pd.notna(site_prest):\n",
        "                self.site_lookup[str(site_prest).strip().lower()] = {\n",
        "                    'nom_site': row.get('Nom site Urbyn'),\n",
        "                    'code_prestation': row.get('Code de la prestation'),\n",
        "                    'prestataire': row.get('Nom prestataire (FORMULE)')\n",
        "                }\n",
        "\n",
        "    def map_dechet_prestataire_to_fin(self, dechet_prest):\n",
        "        \"\"\"Map Déchets prestataire → Déchet fin\"\"\"\n",
        "        if pd.isna(dechet_prest) or str(dechet_prest).strip() == '':\n",
        "            return self.default_dechet_fin\n",
        "\n",
        "        key = str(dechet_prest).strip().lower()\n",
        "        return self.dechet_lookup.get(key, self.default_dechet_fin)\n",
        "\n",
        "    def map_dechet_fin_to_agrege(self, dechet_fin):\n",
        "        \"\"\"Map Déchet fin → Déchets agrégé\"\"\"\n",
        "        if pd.isna(dechet_fin):\n",
        "            return None\n",
        "\n",
        "        key = str(dechet_fin).strip().lower()\n",
        "        return self.dechet_to_agrege.get(key)\n",
        "\n",
        "    def map_traitement(self, dechets_agrege, code_prest):\n",
        "        \"\"\"Map Déchets agrégé + Code prestataire → Code final + Traitement\"\"\"\n",
        "        if pd.isna(dechets_agrege):\n",
        "            return None, None\n",
        "\n",
        "        # Try with code\n",
        "        if pd.notna(code_prest) and str(code_prest).strip():\n",
        "            lookup_key = f\"{dechets_agrege}{str(code_prest).strip()}\"\n",
        "            if lookup_key in self.traitement_lookup:\n",
        "                result = self.traitement_lookup[lookup_key]\n",
        "                return result['code'], result['traitement']\n",
        "\n",
        "        # Try without code (default for this waste type)\n",
        "        if dechets_agrege in self.traitement_lookup:\n",
        "            result = self.traitement_lookup[dechets_agrege]\n",
        "            return result['code'], result['traitement']\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def map_site(self, site_name):\n",
        "        \"\"\"Map site name to Urbyn site info\"\"\"\n",
        "        if pd.isna(site_name):\n",
        "            return None\n",
        "\n",
        "        key = str(site_name).strip().lower()\n",
        "\n",
        "        # Exact match\n",
        "        if key in self.site_lookup:\n",
        "            return self.site_lookup[key]\n",
        "\n",
        "        # Partial match\n",
        "        for lookup_key, value in self.site_lookup.items():\n",
        "            if key in lookup_key or lookup_key in key:\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "def read_inputs(input_files):\n",
        "    \"\"\"Read all input files\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING INPUT FILES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    dfs = []\n",
        "    for name, filepath in input_files.items():\n",
        "        if os.path.exists(filepath):\n",
        "            df = pd.read_excel(filepath, sheet_name=0)\n",
        "            df['_source_file'] = name\n",
        "            print(f\"✓ {name}: {len(df)} rows\")\n",
        "            dfs.append(df)\n",
        "        else:\n",
        "            print(f\"⚠ File not found: {filepath}\")\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"No input files found!\")\n",
        "\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "\n",
        "def read_template(template_file):\n",
        "    \"\"\"Read template for column structure\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING TEMPLATE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df = pd.read_excel(template_file, sheet_name=0, header=TEMPLATE_HEADER_ROW)\n",
        "    print(f\"✓ Template: {len(df.columns)} columns\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def transform(df_input, df_template, mapper):\n",
        "    \"\"\"Transform input data to output format\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRANSFORMING DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    output_rows = []\n",
        "\n",
        "    for idx, row in df_input.iterrows():\n",
        "        # === SITE MAPPING ===\n",
        "        site_name = row.get('Lieu de collecte', '')\n",
        "        site_info = mapper.map_site(site_name)\n",
        "\n",
        "        # === WASTE MAPPING ===\n",
        "        dechets_prest = row.get('Matière', '')\n",
        "        dechet_fin = mapper.map_dechet_prestataire_to_fin(dechets_prest)\n",
        "        dechets_agrege = mapper.map_dechet_fin_to_agrege(dechet_fin)\n",
        "\n",
        "        # === TREATMENT MAPPING ===\n",
        "        code_prest = row.get('Code de traitement', '')\n",
        "        code_final, traitement = mapper.map_traitement(dechets_agrege, code_prest)\n",
        "\n",
        "        # === WEIGHT CONVERSION (tonnes → kg) ===\n",
        "        poids = row.get('Poids', 0)\n",
        "        if pd.isna(poids):\n",
        "            poids = 0\n",
        "        unite = str(row.get('Unité poids', '')).lower()\n",
        "        masse_kg = float(poids) * 1000 if 'tonne' in unite else float(poids)\n",
        "\n",
        "        # === BUILD OUTPUT ROW ===\n",
        "        new_row = {\n",
        "            'Libellé': None,\n",
        "            'Groupe': GROUPE,\n",
        "            'Code site': site_info['nom_site'].split(' - ')[0] if site_info and pd.notna(site_info.get('nom_site')) and ' - ' in str(site_info.get('nom_site', '')) else None,\n",
        "            'Nom du site': site_info['nom_site'] if site_info else 'Site inconnu',\n",
        "            'Nom du client': CLIENT,\n",
        "            'Type de porteur': 'FM',\n",
        "            'Commentaire mouvement': None,\n",
        "            'Code de la prestation': site_info['code_prestation'] if site_info else None,\n",
        "            'Prestataire': site_info['prestataire'] if site_info else row.get('_source_file', GROUPE_PRESTATAIRE),\n",
        "            'Groupe de Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Type de prestataire': TYPE_PRESTATAIRE,\n",
        "            'Périodicité': 'Jour',\n",
        "            'Date début registre': row.get('Date de réalisation'),\n",
        "            'Date fin registre': row.get('Date de réalisation'),\n",
        "            'Code déchet prestataire': row.get('COD CED'),\n",
        "            'Déchet fin': dechet_fin,\n",
        "            'Déchets agrégé': dechets_agrege,\n",
        "            'Déchets prestataire': dechets_prest if pd.notna(dechets_prest) else dechet_fin,\n",
        "            'Masse totale (kg)': masse_kg,\n",
        "            'Nombre de contenants': row.get('Quantité facturée', 1),\n",
        "            'Volume contenant (L)': None,\n",
        "            'Type de contenant': row.get('Matériel'),\n",
        "            'Volume total (L)': None,\n",
        "            'Nature de quantités collectées': 'Masse',\n",
        "            'Qualité quantités': 'Document prestataire',\n",
        "            'Précision estimations des quantités': None,\n",
        "            'Traitement': traitement,\n",
        "            'Traitement prestataire': None,\n",
        "            'Code traitement': code_final if code_final else code_prest,\n",
        "            'Code traitement prestataire': code_prest,\n",
        "            'Qualité du Traitement': 'Document prestataire',\n",
        "            \"N° de BSD/BSDD\": row.get(\"N° de bon d'enlèvement\"),\n",
        "            'N° de recépissé': row.get('Récépissé du transporteur'),\n",
        "            'Transporteur': row.get('Nom du transporteur'),\n",
        "            'Transporteur prestataire': row.get('Nom du transporteur'),\n",
        "            \"Plaque d'immatriculation\": None,\n",
        "            'Exutoire intermédiaire': None,\n",
        "            'Exutoire intermédiaire prestataire': None,\n",
        "            \"Qualité de l'exutoire intermédiaire\": None,\n",
        "            'Exutoire final': row.get('Exutoire'),\n",
        "            'Exutoire final prestataire': row.get('Exutoire'),\n",
        "            \"Qualité de l'exutoire final\": 'Document prestataire',\n",
        "            'Période de clôture': None,\n",
        "            'Statut du mouvement': 'Réalisée',\n",
        "            'Commentaire': row.get('Votre commentaire'),\n",
        "        }\n",
        "\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    # Create DataFrame and align with template columns\n",
        "    df_output = pd.DataFrame(output_rows)\n",
        "\n",
        "    for col in df_template.columns:\n",
        "        if col not in df_output.columns:\n",
        "            df_output[col] = None\n",
        "\n",
        "    df_output = df_output[df_template.columns]\n",
        "\n",
        "    print(f\"✓ Transformed {len(df_output)} rows\")\n",
        "    return df_output\n",
        "\n",
        "\n",
        "def save_output(df_output, output_file):\n",
        "    \"\"\"Save output file\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING OUTPUT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df_output.to_excel(output_file, sheet_name=OUTPUT_SHEET, index=False)\n",
        "    print(f\"✓ Saved: {output_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"VEOLIA ETL TRANSFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        mapper = ETLMapper(ETL_FILE, PRESTATAIRE_PATTERN)\n",
        "        df_input = read_inputs(INPUT_FILES)\n",
        "        df_template = read_template(TEMPLATE_FILE)\n",
        "        df_output = transform(df_input, df_template, mapper)\n",
        "        save_output(df_output, OUTPUT_FILE)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✓✓✓ SUCCESS ✓✓✓\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suez"
      ],
      "metadata": {
        "id": "pzkMinJcPODF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Suez ETL Transformation Script (Google Colab Version)\n",
        "Transforms Suez waste registry data to Urbyn aggregated format.\n",
        "\n",
        "Uses ETL mapping file DIRECTLY:\n",
        "- Déchet sheet: Déchets prestataire → Déchet fin\n",
        "- Paramètres sheet: Déchet fin → Déchets agrégé\n",
        "- Traitement générique sheet: Treatment code mapping\n",
        "- Site sheet: Site name mapping\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR ENVIRONMENT\n",
        "# =============================================================================\n",
        "INPUT_FILES = {\n",
        "    'Suez - Midi Pyrénées': '/content/Reporting_Suez - Midi Pyrénées_Capgemini_03_25.xlsx',\n",
        "    'Suez - Montpellier': '/content/Reporting_Suez - Montpellier_Capgemini_03_25.xlsx',\n",
        "}\n",
        "TEMPLATE_FILE = '/content/Modèle vierge de Registre des déchets et Reporting des coûts SLIMAN.xlsx'\n",
        "ETL_FILE = '/content/ETL _ Mapping registre déchets prestataire vers Urbyn.xlsx'\n",
        "OUTPUT_FILE = '/content/Suez_Registre_Agrege_OUTPUT.xlsx'\n",
        "\n",
        "# Prestataire search pattern (partial match in ETL)\n",
        "PRESTATAIRE_PATTERN = 'Suez'\n",
        "\n",
        "TEMPLATE_HEADER_ROW = 8\n",
        "INPUT_HEADER_ROW = 6  # Suez has sub-headers\n",
        "OUTPUT_SHEET = 'Registre des déchets (Mouvement'\n",
        "\n",
        "CLIENT = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE = \"Capgemini\"\n",
        "GROUPE_PRESTATAIRE = \"Suez\"\n",
        "TYPE_PRESTATAIRE = \"Privé\"\n",
        "\n",
        "\n",
        "class ETLMapper:\n",
        "    \"\"\"Loads and applies mappings from the ETL file\"\"\"\n",
        "\n",
        "    def __init__(self, etl_file, prestataire_pattern):\n",
        "        self.etl_file = etl_file\n",
        "        self.prestataire_pattern = prestataire_pattern\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"LOADING ETL MAPPINGS FOR: {prestataire_pattern}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.load_dechet_mapping()\n",
        "        self.load_dechet_to_agrege_mapping()\n",
        "        self.load_traitement_mapping()\n",
        "        self.load_site_mapping()\n",
        "\n",
        "    def load_dechet_mapping(self):\n",
        "        \"\"\"Load Déchet sheet: Déchets prestataire → Déchet fin\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Déchet')\n",
        "\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(\n",
        "            self.prestataire_pattern, case=False, na=False\n",
        "        )\n",
        "        df_filtered = df[mask]\n",
        "\n",
        "        print(f\"✓ Déchet sheet: {len(df_filtered)} mappings for '{self.prestataire_pattern}'\")\n",
        "\n",
        "        self.dechet_lookup = {}\n",
        "        self.default_dechet_fin = None\n",
        "\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            prest = row.get('Nom des déchets prestataire')\n",
        "            urbyn = row.get('Nom des déchets Urbyn')\n",
        "\n",
        "            if pd.isna(prest):\n",
        "                if pd.notna(urbyn):\n",
        "                    self.default_dechet_fin = urbyn\n",
        "            elif pd.notna(urbyn):\n",
        "                self.dechet_lookup[str(prest).strip().lower()] = urbyn\n",
        "\n",
        "        print(f\"  - {len(self.dechet_lookup)} explicit mappings\")\n",
        "        print(f\"  - Default (for empty): {self.default_dechet_fin}\")\n",
        "\n",
        "    def load_dechet_to_agrege_mapping(self):\n",
        "        \"\"\"Load Paramètres sheet: Déchet fin (Name) → Déchets agrégé (Category)\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Paramètres')\n",
        "\n",
        "        self.dechet_to_agrege = {}\n",
        "        for _, row in df[['Category', 'Name']].dropna().iterrows():\n",
        "            name = str(row['Name']).strip().lower()\n",
        "            category = str(row['Category']).strip()\n",
        "            self.dechet_to_agrege[name] = category\n",
        "\n",
        "        print(f\"✓ Paramètres sheet: {len(self.dechet_to_agrege)} Déchet fin → Déchets agrégé mappings\")\n",
        "\n",
        "    def load_traitement_mapping(self):\n",
        "        \"\"\"Load Traitement générique sheet\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Traitement générique')\n",
        "\n",
        "        self.traitement_lookup = {}\n",
        "        for _, row in df.iterrows():\n",
        "            key = str(row.get('Concatener déchet & code de traitement prestataire', '')).strip()\n",
        "            if key:\n",
        "                self.traitement_lookup[key] = {\n",
        "                    'code': row.get('Code traitement retraité'),\n",
        "                    'traitement': row.get('Traitement')\n",
        "                }\n",
        "\n",
        "        print(f\"✓ Traitement générique: {len(self.traitement_lookup)} treatment mappings\")\n",
        "\n",
        "    def load_site_mapping(self):\n",
        "        \"\"\"Load Site sheet\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Site')\n",
        "\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(\n",
        "            self.prestataire_pattern, case=False, na=False\n",
        "        )\n",
        "        df_filtered = df[mask]\n",
        "\n",
        "        print(f\"✓ Site sheet: {len(df_filtered)} sites for '{self.prestataire_pattern}'\")\n",
        "\n",
        "        self.site_lookup = {}\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            site_prest = row.get('Nom site prestataire')\n",
        "            if pd.notna(site_prest):\n",
        "                self.site_lookup[str(site_prest).strip().lower()] = {\n",
        "                    'nom_site': row.get('Nom site Urbyn'),\n",
        "                    'code_prestation': row.get('Code de la prestation'),\n",
        "                    'prestataire': row.get('Nom prestataire (FORMULE)')\n",
        "                }\n",
        "\n",
        "    def map_dechet_prestataire_to_fin(self, dechet_prest):\n",
        "        \"\"\"Map Déchets prestataire → Déchet fin\"\"\"\n",
        "        if pd.isna(dechet_prest) or str(dechet_prest).strip() == '':\n",
        "            return self.default_dechet_fin\n",
        "\n",
        "        key = str(dechet_prest).strip().lower()\n",
        "        return self.dechet_lookup.get(key, self.default_dechet_fin)\n",
        "\n",
        "    def map_dechet_fin_to_agrege(self, dechet_fin):\n",
        "        \"\"\"Map Déchet fin → Déchets agrégé\"\"\"\n",
        "        if pd.isna(dechet_fin):\n",
        "            return None\n",
        "\n",
        "        key = str(dechet_fin).strip().lower()\n",
        "        return self.dechet_to_agrege.get(key)\n",
        "\n",
        "    def map_traitement(self, dechets_agrege, code_prest):\n",
        "        \"\"\"Map Déchets agrégé + Code prestataire → Code final + Traitement\"\"\"\n",
        "        if pd.isna(dechets_agrege):\n",
        "            return None, None\n",
        "\n",
        "        if pd.notna(code_prest) and str(code_prest).strip():\n",
        "            lookup_key = f\"{dechets_agrege}{str(code_prest).strip()}\"\n",
        "            if lookup_key in self.traitement_lookup:\n",
        "                result = self.traitement_lookup[lookup_key]\n",
        "                return result['code'], result['traitement']\n",
        "\n",
        "        if dechets_agrege in self.traitement_lookup:\n",
        "            result = self.traitement_lookup[dechets_agrege]\n",
        "            return result['code'], result['traitement']\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def map_site(self, site_name):\n",
        "        \"\"\"Map site name to Urbyn site info\"\"\"\n",
        "        if pd.isna(site_name):\n",
        "            return None\n",
        "\n",
        "        key = str(site_name).strip().lower()\n",
        "\n",
        "        if key in self.site_lookup:\n",
        "            return self.site_lookup[key]\n",
        "\n",
        "        for lookup_key, value in self.site_lookup.items():\n",
        "            if key in lookup_key or lookup_key in key:\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "def read_inputs(input_files):\n",
        "    \"\"\"Read all Suez input files\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING INPUT FILES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    dfs = []\n",
        "    for name, filepath in input_files.items():\n",
        "        if os.path.exists(filepath):\n",
        "            # Suez files have header at row 6\n",
        "            df = pd.read_excel(filepath, sheet_name='Registre déchets', header=INPUT_HEADER_ROW)\n",
        "            # Filter out empty rows\n",
        "            df = df[df['Type'].notna()].reset_index(drop=True)\n",
        "            df['_source_file'] = name\n",
        "            print(f\"✓ {name}: {len(df)} rows\")\n",
        "            dfs.append(df)\n",
        "        else:\n",
        "            print(f\"⚠ File not found: {filepath}\")\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"No input files found!\")\n",
        "\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "\n",
        "def read_template(template_file):\n",
        "    \"\"\"Read template for column structure\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING TEMPLATE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df = pd.read_excel(template_file, sheet_name=0, header=TEMPLATE_HEADER_ROW)\n",
        "    print(f\"✓ Template: {len(df.columns)} columns\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def transform(df_input, df_template, mapper):\n",
        "    \"\"\"Transform input data to output format\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRANSFORMING DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    output_rows = []\n",
        "\n",
        "    for idx, row in df_input.iterrows():\n",
        "        # === SITE MAPPING ===\n",
        "        site_name = row.get('Nom du site', '')\n",
        "        site_info = mapper.map_site(site_name)\n",
        "\n",
        "        # === WASTE MAPPING ===\n",
        "        dechets_prest = row.get('Matière', '')\n",
        "        dechet_fin = mapper.map_dechet_prestataire_to_fin(dechets_prest)\n",
        "        dechets_agrege = mapper.map_dechet_fin_to_agrege(dechet_fin)\n",
        "\n",
        "        # === TREATMENT MAPPING ===\n",
        "        code_prest = row.get('Code R/D', '')\n",
        "        code_final, traitement = mapper.map_traitement(dechets_agrege, code_prest)\n",
        "\n",
        "        # === WEIGHT CONVERSION (Suez uses tonnes) ===\n",
        "        poids = row.get('Qté pesée (dont estimée)', 0)\n",
        "        if pd.isna(poids):\n",
        "            poids = 0\n",
        "        masse_kg = float(poids) * 1000  # Suez is always in tonnes\n",
        "\n",
        "        # === BUILD OUTPUT ROW ===\n",
        "        new_row = {\n",
        "            'Libellé': None,\n",
        "            'Groupe': GROUPE,\n",
        "            'Code site': site_info['nom_site'].split(' - ')[0] if site_info and pd.notna(site_info.get('nom_site')) and ' - ' in str(site_info.get('nom_site', '')) else None,\n",
        "            'Nom du site': site_info['nom_site'] if site_info else 'Site inconnu',\n",
        "            'Nom du client': CLIENT,\n",
        "            'Type de porteur': 'FM',\n",
        "            'Commentaire mouvement': None,\n",
        "            'Code de la prestation': site_info['code_prestation'] if site_info else None,\n",
        "            'Prestataire': site_info['prestataire'] if site_info else row.get('_source_file', GROUPE_PRESTATAIRE),\n",
        "            'Groupe de Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Type de prestataire': TYPE_PRESTATAIRE,\n",
        "            'Périodicité': 'Jour',\n",
        "            'Date début registre': row.get(\"Date de l'expédition\"),\n",
        "            'Date fin registre': row.get(\"Date de l'expédition\"),\n",
        "            'Code déchet prestataire': row.get('Code déchet (CED)'),\n",
        "            'Déchet fin': dechet_fin,\n",
        "            'Déchets agrégé': dechets_agrege,\n",
        "            'Déchets prestataire': dechets_prest if pd.notna(dechets_prest) else dechet_fin,\n",
        "            'Masse totale (kg)': masse_kg,\n",
        "            'Nombre de contenants': row.get('Qté de matériel collectée', 1),\n",
        "            'Volume contenant (L)': row.get('Volume du matériel'),\n",
        "            'Type de contenant': None,\n",
        "            'Volume total (L)': None,\n",
        "            'Nature de quantités collectées': 'Masse',\n",
        "            'Qualité quantités': 'Document prestataire',\n",
        "            'Précision estimations des quantités': None,\n",
        "            'Traitement': traitement,\n",
        "            'Traitement prestataire': row.get('Libellé R/D'),\n",
        "            'Code traitement': code_final if code_final else code_prest,\n",
        "            'Code traitement prestataire': code_prest,\n",
        "            'Qualité du Traitement': 'Document prestataire',\n",
        "            'N° de BSD/BSDD': row.get('N° du BSD'),\n",
        "            'N° de recépissé': row.get('N° récépissé'),\n",
        "            'Transporteur': row.get('Nom.1'),\n",
        "            'Transporteur prestataire': row.get('Nom.1'),\n",
        "            \"Plaque d'immatriculation\": None,\n",
        "            'Exutoire intermédiaire': None,\n",
        "            'Exutoire intermédiaire prestataire': None,\n",
        "            \"Qualité de l'exutoire intermédiaire\": None,\n",
        "            'Exutoire final': row.get(\"Nom de l'installation\"),\n",
        "            'Exutoire final prestataire': row.get(\"Nom de l'installation\"),\n",
        "            \"Qualité de l'exutoire final\": 'Document prestataire',\n",
        "            'Période de clôture': None,\n",
        "            'Statut du mouvement': 'Réalisée',\n",
        "            'Commentaire': None,\n",
        "        }\n",
        "\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    df_output = pd.DataFrame(output_rows)\n",
        "\n",
        "    for col in df_template.columns:\n",
        "        if col not in df_output.columns:\n",
        "            df_output[col] = None\n",
        "\n",
        "    df_output = df_output[df_template.columns]\n",
        "\n",
        "    print(f\"✓ Transformed {len(df_output)} rows\")\n",
        "    return df_output\n",
        "\n",
        "\n",
        "def save_output(df_output, output_file):\n",
        "    \"\"\"Save output file\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING OUTPUT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df_output.to_excel(output_file, sheet_name=OUTPUT_SHEET, index=False)\n",
        "    print(f\"✓ Saved: {output_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUEZ ETL TRANSFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        mapper = ETLMapper(ETL_FILE, PRESTATAIRE_PATTERN)\n",
        "        df_input = read_inputs(INPUT_FILES)\n",
        "        df_template = read_template(TEMPLATE_FILE)\n",
        "        df_output = transform(df_input, df_template, mapper)\n",
        "        save_output(df_output, OUTPUT_FILE)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✓✓✓ SUCCESS ✓✓✓\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3H0hCS1PQ9g",
        "outputId": "0a91611e-bf98-412b-b988-a9acd6e008e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SUEZ ETL TRANSFORMATION\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING ETL MAPPINGS FOR: Suez\n",
            "================================================================================\n",
            "✓ Déchet sheet: 34 mappings for 'Suez'\n",
            "  - 22 explicit mappings\n",
            "  - Default (for empty): None\n",
            "✓ Paramètres sheet: 121 Déchet fin → Déchets agrégé mappings\n",
            "✓ Traitement générique: 169 treatment mappings\n",
            "✓ Site sheet: 9 sites for 'Suez'\n",
            "\n",
            "================================================================================\n",
            "READING INPUT FILES\n",
            "================================================================================\n",
            "✓ Suez - Midi Pyrénées: 94 rows\n",
            "✓ Suez - Montpellier: 9 rows\n",
            "\n",
            "================================================================================\n",
            "READING TEMPLATE\n",
            "================================================================================\n",
            "✓ Template: 64 columns\n",
            "\n",
            "================================================================================\n",
            "TRANSFORMING DATA\n",
            "================================================================================\n",
            "✓ Transformed 103 rows\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "✓ Saved: /content/Suez_Registre_Agrege_OUTPUT.xlsx\n",
            "\n",
            "================================================================================\n",
            "✓✓✓ SUCCESS ✓✓✓\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apeyron"
      ],
      "metadata": {
        "id": "d0PdKUhUPiol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Apeyron ETL Transformation Script (Google Colab Version)\n",
        "Transforms Apeyron waste registry data to Urbyn aggregated format.\n",
        "\n",
        "Uses ETL mapping file DIRECTLY:\n",
        "- Déchet sheet: Déchets prestataire → Déchet fin\n",
        "- Paramètres sheet: Déchet fin → Déchets agrégé\n",
        "- Traitement générique sheet: Treatment code mapping\n",
        "- Site sheet: Site name mapping\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import re\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR ENVIRONMENT\n",
        "# =============================================================================\n",
        "INPUT_FILE = '/content/Reporting_Apeyron_Capgemini_03_25.xlsx'\n",
        "TEMPLATE_FILE = '/content/Modèle vierge de Registre des déchets et Reporting des coûts SLIMAN.xlsx'\n",
        "ETL_FILE = '/content/ETL _ Mapping registre déchets prestataire vers Urbyn.xlsx'\n",
        "OUTPUT_FILE = '/content/Apeyron_Registre_Agrege_OUTPUT.xlsx'\n",
        "\n",
        "# Prestataire search pattern (partial match in ETL)\n",
        "PRESTATAIRE_PATTERN = 'Apeyron'\n",
        "\n",
        "TEMPLATE_HEADER_ROW = 8\n",
        "INPUT_HEADER_ROW = 1  # Apeyron header row\n",
        "OUTPUT_SHEET = 'Registre des déchets (Mouvement'\n",
        "\n",
        "CLIENT = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE = \"Capgemini\"\n",
        "GROUPE_PRESTATAIRE = \"Apeyron Environnement\"\n",
        "TYPE_PRESTATAIRE = \"Privé\"\n",
        "\n",
        "\n",
        "class ETLMapper:\n",
        "    \"\"\"Loads and applies mappings from the ETL file\"\"\"\n",
        "\n",
        "    def __init__(self, etl_file, prestataire_pattern):\n",
        "        self.etl_file = etl_file\n",
        "        self.prestataire_pattern = prestataire_pattern\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"LOADING ETL MAPPINGS FOR: {prestataire_pattern}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.load_dechet_mapping()\n",
        "        self.load_dechet_to_agrege_mapping()\n",
        "        self.load_traitement_mapping()\n",
        "        self.load_site_mapping()\n",
        "\n",
        "    def load_dechet_mapping(self):\n",
        "        \"\"\"Load Déchet sheet: Déchets prestataire → Déchet fin\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Déchet')\n",
        "\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(\n",
        "            self.prestataire_pattern, case=False, na=False\n",
        "        )\n",
        "        df_filtered = df[mask]\n",
        "\n",
        "        print(f\"✓ Déchet sheet: {len(df_filtered)} mappings for '{self.prestataire_pattern}'\")\n",
        "\n",
        "        self.dechet_lookup = {}\n",
        "        self.default_dechet_fin = None\n",
        "\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            prest = row.get('Nom des déchets prestataire')\n",
        "            urbyn = row.get('Nom des déchets Urbyn')\n",
        "\n",
        "            if pd.isna(prest):\n",
        "                if pd.notna(urbyn):\n",
        "                    self.default_dechet_fin = urbyn\n",
        "            elif pd.notna(urbyn):\n",
        "                self.dechet_lookup[str(prest).strip().lower()] = urbyn\n",
        "\n",
        "        print(f\"  - {len(self.dechet_lookup)} explicit mappings\")\n",
        "        print(f\"  - Default (for empty): {self.default_dechet_fin}\")\n",
        "\n",
        "    def load_dechet_to_agrege_mapping(self):\n",
        "        \"\"\"Load Paramètres sheet: Déchet fin (Name) → Déchets agrégé (Category)\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Paramètres')\n",
        "\n",
        "        self.dechet_to_agrege = {}\n",
        "        for _, row in df[['Category', 'Name']].dropna().iterrows():\n",
        "            name = str(row['Name']).strip().lower()\n",
        "            category = str(row['Category']).strip()\n",
        "            self.dechet_to_agrege[name] = category\n",
        "\n",
        "        print(f\"✓ Paramètres sheet: {len(self.dechet_to_agrege)} Déchet fin → Déchets agrégé mappings\")\n",
        "\n",
        "    def load_traitement_mapping(self):\n",
        "        \"\"\"Load Traitement générique sheet\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Traitement générique')\n",
        "\n",
        "        self.traitement_lookup = {}\n",
        "        for _, row in df.iterrows():\n",
        "            key = str(row.get('Concatener déchet & code de traitement prestataire', '')).strip()\n",
        "            if key:\n",
        "                self.traitement_lookup[key] = {\n",
        "                    'code': row.get('Code traitement retraité'),\n",
        "                    'traitement': row.get('Traitement')\n",
        "                }\n",
        "\n",
        "        print(f\"✓ Traitement générique: {len(self.traitement_lookup)} treatment mappings\")\n",
        "\n",
        "    def load_site_mapping(self):\n",
        "        \"\"\"Load Site sheet\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Site')\n",
        "\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(\n",
        "            self.prestataire_pattern, case=False, na=False\n",
        "        )\n",
        "        df_filtered = df[mask]\n",
        "\n",
        "        print(f\"✓ Site sheet: {len(df_filtered)} sites for '{self.prestataire_pattern}'\")\n",
        "\n",
        "        self.site_lookup = {}\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            site_prest = row.get('Nom site prestataire')\n",
        "            if pd.notna(site_prest):\n",
        "                self.site_lookup[str(site_prest).strip().lower()] = {\n",
        "                    'nom_site': row.get('Nom site Urbyn'),\n",
        "                    'code_prestation': row.get('Code de la prestation'),\n",
        "                    'prestataire': row.get('Nom prestataire (FORMULE)')\n",
        "                }\n",
        "\n",
        "    def map_dechet_prestataire_to_fin(self, dechet_prest):\n",
        "        \"\"\"Map Déchets prestataire → Déchet fin\"\"\"\n",
        "        if pd.isna(dechet_prest) or str(dechet_prest).strip() == '':\n",
        "            return self.default_dechet_fin\n",
        "\n",
        "        key = str(dechet_prest).strip().lower()\n",
        "        return self.dechet_lookup.get(key, self.default_dechet_fin)\n",
        "\n",
        "    def map_dechet_fin_to_agrege(self, dechet_fin):\n",
        "        \"\"\"Map Déchet fin → Déchets agrégé\"\"\"\n",
        "        if pd.isna(dechet_fin):\n",
        "            return None\n",
        "\n",
        "        key = str(dechet_fin).strip().lower()\n",
        "        return self.dechet_to_agrege.get(key)\n",
        "\n",
        "    def map_traitement(self, dechets_agrege, code_prest):\n",
        "        \"\"\"Map Déchets agrégé + Code prestataire → Code final + Traitement\"\"\"\n",
        "        if pd.isna(dechets_agrege):\n",
        "            return None, None\n",
        "\n",
        "        if pd.notna(code_prest) and str(code_prest).strip():\n",
        "            lookup_key = f\"{dechets_agrege}{str(code_prest).strip()}\"\n",
        "            if lookup_key in self.traitement_lookup:\n",
        "                result = self.traitement_lookup[lookup_key]\n",
        "                return result['code'], result['traitement']\n",
        "\n",
        "        if dechets_agrege in self.traitement_lookup:\n",
        "            result = self.traitement_lookup[dechets_agrege]\n",
        "            return result['code'], result['traitement']\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def map_site(self, site_name):\n",
        "        \"\"\"Map site name to Urbyn site info\"\"\"\n",
        "        if pd.isna(site_name):\n",
        "            return None\n",
        "\n",
        "        key = str(site_name).strip().lower()\n",
        "\n",
        "        if key in self.site_lookup:\n",
        "            return self.site_lookup[key]\n",
        "\n",
        "        for lookup_key, value in self.site_lookup.items():\n",
        "            if key in lookup_key or lookup_key in key:\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_volume_from_container(self, container_code):\n",
        "        \"\"\"Extract volume in liters from container code (e.g., BAC035 → 35)\"\"\"\n",
        "        if pd.isna(container_code):\n",
        "            return None\n",
        "\n",
        "        container_str = str(container_code).upper()\n",
        "        match = re.search(r'BAC\\s*0*(\\d+)', container_str)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_container_type_name(self, volume_liters):\n",
        "        \"\"\"Get standardized container type name from volume\"\"\"\n",
        "        if volume_liters is None:\n",
        "            return None\n",
        "\n",
        "        names = {\n",
        "            35: 'Bac 2 roues - 35 L',\n",
        "            120: 'Bac 2 roues - 120 L',\n",
        "            240: 'Bac 2 roues - 240 L',\n",
        "            660: 'Bac 4 roues - 660 L',\n",
        "            770: 'Bac 4 roues - 770 L',\n",
        "            1100: 'Bac 4 roues - 1100 L',\n",
        "        }\n",
        "        return names.get(volume_liters, f'Bac - {volume_liters} L')\n",
        "\n",
        "\n",
        "def read_input(input_file):\n",
        "    \"\"\"Read Apeyron input file\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING INPUT FILE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df = pd.read_excel(input_file, sheet_name=0, header=INPUT_HEADER_ROW)\n",
        "    df = df[df['Date'].notna()].reset_index(drop=True)\n",
        "\n",
        "    print(f\"✓ Loaded: {len(df)} rows\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def read_template(template_file):\n",
        "    \"\"\"Read template for column structure\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING TEMPLATE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df = pd.read_excel(template_file, sheet_name=0, header=TEMPLATE_HEADER_ROW)\n",
        "    print(f\"✓ Template: {len(df.columns)} columns\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def transform(df_input, df_template, mapper):\n",
        "    \"\"\"Transform input data to output format\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRANSFORMING DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    output_rows = []\n",
        "\n",
        "    for idx, row in df_input.iterrows():\n",
        "        # === SITE MAPPING ===\n",
        "        site_name = row.get('Nom Etablissement', '')\n",
        "        site_info = mapper.map_site(site_name)\n",
        "\n",
        "        if site_info is None:\n",
        "            # Skip rows without site mapping\n",
        "            continue\n",
        "\n",
        "        # === WASTE MAPPING (Apeyron usually has no waste column) ===\n",
        "        dechets_prest = None\n",
        "        for col in ['Type de matière', 'Matière', 'Type déchet', 'Déchet']:\n",
        "            if col in row.index and pd.notna(row.get(col)):\n",
        "                dechets_prest = row.get(col)\n",
        "                break\n",
        "\n",
        "        dechet_fin = mapper.map_dechet_prestataire_to_fin(dechets_prest)\n",
        "        dechets_agrege = mapper.map_dechet_fin_to_agrege(dechet_fin)\n",
        "\n",
        "        # === TREATMENT MAPPING ===\n",
        "        code_prest = None\n",
        "        for col in ['Code traitement', 'Traitement', 'Code R/D']:\n",
        "            if col in row.index and pd.notna(row.get(col)):\n",
        "                code_prest = row.get(col)\n",
        "                break\n",
        "\n",
        "        code_final, traitement = mapper.map_traitement(dechets_agrege, code_prest)\n",
        "\n",
        "        # === WEIGHT (Apeyron already in kg) ===\n",
        "        masse_kg = row.get('Poids Total Kg', 0)\n",
        "        if pd.isna(masse_kg):\n",
        "            masse_kg = 0\n",
        "\n",
        "        # === CONTAINER ===\n",
        "        nb_contenants = row.get('Nb Bac', 1)\n",
        "        if pd.isna(nb_contenants):\n",
        "            nb_contenants = 1\n",
        "\n",
        "        container_code = row.get('Volume Bac', '')\n",
        "        volume_contenant = mapper.get_volume_from_container(container_code)\n",
        "        type_contenant = mapper.get_container_type_name(volume_contenant)\n",
        "        volume_total = volume_contenant * nb_contenants if volume_contenant else None\n",
        "\n",
        "        # === BUILD OUTPUT ROW ===\n",
        "        new_row = {\n",
        "            'Libellé': None,\n",
        "            'Groupe': GROUPE,\n",
        "            'Code site': site_info['nom_site'].split(' - ')[0] if site_info and pd.notna(site_info.get('nom_site')) and ' - ' in str(site_info.get('nom_site', '')) else None,\n",
        "            'Nom du site': site_info['nom_site'] if site_info else 'Site inconnu',\n",
        "            'Nom du client': CLIENT,\n",
        "            'Type de porteur': 'FM',\n",
        "            'Commentaire mouvement': None,\n",
        "            'Code de la prestation': site_info['code_prestation'] if site_info else None,\n",
        "            'Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Groupe de Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Type de prestataire': TYPE_PRESTATAIRE,\n",
        "            'Périodicité': 'Jour',\n",
        "            'Date début registre': row.get('Date'),\n",
        "            'Date fin registre': row.get('Date'),\n",
        "            'Code déchet prestataire': '20_01_08',\n",
        "            'Déchet fin': dechet_fin,\n",
        "            'Déchets agrégé': dechets_agrege,\n",
        "            'Déchets prestataire': dechets_prest if pd.notna(dechets_prest) else dechet_fin,\n",
        "            'Masse totale (kg)': masse_kg,\n",
        "            'Nombre de contenants': nb_contenants,\n",
        "            'Volume contenant (L)': volume_contenant,\n",
        "            'Type de contenant': type_contenant,\n",
        "            'Volume total (L)': volume_total,\n",
        "            'Nature de quantités collectées': 'Volume et Masse',\n",
        "            'Qualité quantités': 'Document prestataire',\n",
        "            'Précision estimations des quantités': None,\n",
        "            'Traitement': traitement,\n",
        "            'Traitement prestataire': None,\n",
        "            'Code traitement': code_final if code_final else code_prest,\n",
        "            'Code traitement prestataire': code_prest,\n",
        "            'Qualité du Traitement': 'Document prestataire',\n",
        "            'N° de BSD/BSDD': None,\n",
        "            'N° de recépissé': None,\n",
        "            'Transporteur': None,\n",
        "            'Transporteur prestataire': GROUPE_PRESTATAIRE,\n",
        "            \"Plaque d'immatriculation\": None,\n",
        "            'Exutoire intermédiaire': None,\n",
        "            'Exutoire intermédiaire prestataire': 'Exutoire intermédiaire inconnu',\n",
        "            \"Qualité de l'exutoire intermédiaire\": 'Inconnu',\n",
        "            'Exutoire final': None,\n",
        "            'Exutoire final prestataire': None,\n",
        "            \"Qualité de l'exutoire final\": 'Document prestataire',\n",
        "            'Période de clôture': None,\n",
        "            'Statut du mouvement': 'Réalisée',\n",
        "            'Commentaire': None,\n",
        "        }\n",
        "\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    df_output = pd.DataFrame(output_rows)\n",
        "\n",
        "    for col in df_template.columns:\n",
        "        if col not in df_output.columns:\n",
        "            df_output[col] = None\n",
        "\n",
        "    df_output = df_output[df_template.columns]\n",
        "\n",
        "    print(f\"✓ Transformed {len(df_output)} rows\")\n",
        "    return df_output\n",
        "\n",
        "\n",
        "def save_output(df_output, output_file):\n",
        "    \"\"\"Save output file\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING OUTPUT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df_output.to_excel(output_file, sheet_name=OUTPUT_SHEET, index=False)\n",
        "    print(f\"✓ Saved: {output_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"APEYRON ETL TRANSFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        mapper = ETLMapper(ETL_FILE, PRESTATAIRE_PATTERN)\n",
        "        df_input = read_input(INPUT_FILE)\n",
        "        df_template = read_template(TEMPLATE_FILE)\n",
        "        df_output = transform(df_input, df_template, mapper)\n",
        "        save_output(df_output, OUTPUT_FILE)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✓✓✓ SUCCESS ✓✓✓\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IKY66k3PlZJ",
        "outputId": "794eae08-e5ca-4cba-c0a3-c076bee9f3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "APEYRON ETL TRANSFORMATION\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING ETL MAPPINGS FOR: Apeyron\n",
            "================================================================================\n",
            "✓ Déchet sheet: 1 mappings for 'Apeyron'\n",
            "  - 0 explicit mappings\n",
            "  - Default (for empty): Biodéchets\n",
            "✓ Paramètres sheet: 121 Déchet fin → Déchets agrégé mappings\n",
            "✓ Traitement générique: 169 treatment mappings\n",
            "✓ Site sheet: 1 sites for 'Apeyron'\n",
            "\n",
            "================================================================================\n",
            "READING INPUT FILE\n",
            "================================================================================\n",
            "✓ Loaded: 101 rows\n",
            "\n",
            "================================================================================\n",
            "READING TEMPLATE\n",
            "================================================================================\n",
            "✓ Template: 64 columns\n",
            "\n",
            "================================================================================\n",
            "TRANSFORMING DATA\n",
            "================================================================================\n",
            "✓ Transformed 22 rows\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "✓ Saved: /content/Apeyron_Registre_Agrege_OUTPUT.xlsx\n",
            "\n",
            "================================================================================\n",
            "✓✓✓ SUCCESS ✓✓✓\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# les alchimistes"
      ],
      "metadata": {
        "id": "2uqrBiXsPy-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Les Alchimistes ETL Transformation Script (Google Colab Version)\n",
        "Transforms Les Alchimistes waste registry data to Urbyn aggregated format.\n",
        "\n",
        "Uses ETL mapping file DIRECTLY:\n",
        "- Déchet sheet: Déchets prestataire → Déchet fin\n",
        "- Paramètres sheet: Déchet fin → Déchets agrégé\n",
        "- Traitement générique sheet: Treatment code mapping\n",
        "- Site sheet: Site name mapping\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import re\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR ENVIRONMENT\n",
        "# =============================================================================\n",
        "INPUT_FILES = {\n",
        "    'Lesalchimistes - Idf': '/content/Reporting_Lesalchimistes - Idf_Capgemini_03_25.xlsx',\n",
        "    'Lesalchimistes - Lille': '/content/Reporting_Lesalchimistes - Lille_Capgemini_03_25.xlsx',\n",
        "    'Lesalchimistes - Languedoc - Montpellier': '/content/Reporting_Lesalchimistes - Languedoc - Montpellier_Capgemini_03_25.xlsx',\n",
        "}\n",
        "TEMPLATE_FILE = '/content/Modèle vierge de Registre des déchets et Reporting des coûts SLIMAN.xlsx'\n",
        "ETL_FILE = '/content/ETL _ Mapping registre déchets prestataire vers Urbyn.xlsx'\n",
        "OUTPUT_FILE = '/content/LesAlchimistes_Registre_Agrege_OUTPUT.xlsx'\n",
        "\n",
        "# Prestataire search pattern (partial match in ETL)\n",
        "PRESTATAIRE_PATTERN = 'alchimiste'\n",
        "\n",
        "TEMPLATE_HEADER_ROW = 8\n",
        "OUTPUT_SHEET = 'Registre des déchets (Mouvement'\n",
        "\n",
        "CLIENT = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE = \"Capgemini\"\n",
        "GROUPE_PRESTATAIRE = \"Les Alchimistes\"\n",
        "TYPE_PRESTATAIRE = \"Privé\"\n",
        "\n",
        "\n",
        "class ETLMapper:\n",
        "    \"\"\"Loads and applies mappings from the ETL file\"\"\"\n",
        "\n",
        "    def __init__(self, etl_file, prestataire_pattern):\n",
        "        self.etl_file = etl_file\n",
        "        self.prestataire_pattern = prestataire_pattern\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"LOADING ETL MAPPINGS FOR: {prestataire_pattern}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.load_dechet_mapping()\n",
        "        self.load_dechet_to_agrege_mapping()\n",
        "        self.load_traitement_mapping()\n",
        "        self.load_site_mapping()\n",
        "\n",
        "    def load_dechet_mapping(self):\n",
        "        \"\"\"Load Déchet sheet: Déchets prestataire → Déchet fin\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Déchet')\n",
        "\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(\n",
        "            self.prestataire_pattern, case=False, na=False\n",
        "        )\n",
        "        df_filtered = df[mask]\n",
        "\n",
        "        print(f\"✓ Déchet sheet: {len(df_filtered)} mappings for '{self.prestataire_pattern}'\")\n",
        "\n",
        "        self.dechet_lookup = {}\n",
        "        self.default_dechet_fin = None\n",
        "\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            prest = row.get('Nom des déchets prestataire')\n",
        "            urbyn = row.get('Nom des déchets Urbyn')\n",
        "\n",
        "            if pd.isna(prest):\n",
        "                if pd.notna(urbyn):\n",
        "                    self.default_dechet_fin = urbyn\n",
        "            elif pd.notna(urbyn):\n",
        "                self.dechet_lookup[str(prest).strip().lower()] = urbyn\n",
        "\n",
        "        print(f\"  - {len(self.dechet_lookup)} explicit mappings\")\n",
        "        print(f\"  - Default (for empty): {self.default_dechet_fin}\")\n",
        "\n",
        "    def load_dechet_to_agrege_mapping(self):\n",
        "        \"\"\"Load Paramètres sheet: Déchet fin (Name) → Déchets agrégé (Category)\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Paramètres')\n",
        "\n",
        "        self.dechet_to_agrege = {}\n",
        "        for _, row in df[['Category', 'Name']].dropna().iterrows():\n",
        "            name = str(row['Name']).strip().lower()\n",
        "            category = str(row['Category']).strip()\n",
        "            self.dechet_to_agrege[name] = category\n",
        "\n",
        "        print(f\"✓ Paramètres sheet: {len(self.dechet_to_agrege)} Déchet fin → Déchets agrégé mappings\")\n",
        "\n",
        "    def load_traitement_mapping(self):\n",
        "        \"\"\"Load Traitement générique sheet\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Traitement générique')\n",
        "\n",
        "        self.traitement_lookup = {}\n",
        "        for _, row in df.iterrows():\n",
        "            key = str(row.get('Concatener déchet & code de traitement prestataire', '')).strip()\n",
        "            if key:\n",
        "                self.traitement_lookup[key] = {\n",
        "                    'code': row.get('Code traitement retraité'),\n",
        "                    'traitement': row.get('Traitement')\n",
        "                }\n",
        "\n",
        "        print(f\"✓ Traitement générique: {len(self.traitement_lookup)} treatment mappings\")\n",
        "\n",
        "    def load_site_mapping(self):\n",
        "        \"\"\"Load Site sheet\"\"\"\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Site')\n",
        "\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(\n",
        "            self.prestataire_pattern, case=False, na=False\n",
        "        )\n",
        "        df_filtered = df[mask]\n",
        "\n",
        "        print(f\"✓ Site sheet: {len(df_filtered)} sites for '{self.prestataire_pattern}'\")\n",
        "\n",
        "        self.site_lookup = {}\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            site_prest = row.get('Nom site prestataire')\n",
        "            if pd.notna(site_prest):\n",
        "                self.site_lookup[str(site_prest).strip().lower()] = {\n",
        "                    'nom_site': row.get('Nom site Urbyn'),\n",
        "                    'code_prestation': row.get('Code de la prestation'),\n",
        "                    'prestataire': row.get('Nom prestataire (FORMULE)')\n",
        "                }\n",
        "\n",
        "    def map_dechet_prestataire_to_fin(self, dechet_prest):\n",
        "        \"\"\"Map Déchets prestataire → Déchet fin\"\"\"\n",
        "        if pd.isna(dechet_prest) or str(dechet_prest).strip() == '':\n",
        "            return self.default_dechet_fin\n",
        "\n",
        "        key = str(dechet_prest).strip().lower()\n",
        "        return self.dechet_lookup.get(key, self.default_dechet_fin)\n",
        "\n",
        "    def map_dechet_fin_to_agrege(self, dechet_fin):\n",
        "        \"\"\"Map Déchet fin → Déchets agrégé\"\"\"\n",
        "        if pd.isna(dechet_fin):\n",
        "            return None\n",
        "\n",
        "        key = str(dechet_fin).strip().lower()\n",
        "        return self.dechet_to_agrege.get(key)\n",
        "\n",
        "    def map_traitement(self, dechets_agrege, code_prest):\n",
        "        \"\"\"Map Déchets agrégé + Code prestataire → Code final + Traitement\"\"\"\n",
        "        if pd.isna(dechets_agrege):\n",
        "            return None, None\n",
        "\n",
        "        if pd.notna(code_prest) and str(code_prest).strip():\n",
        "            lookup_key = f\"{dechets_agrege}{str(code_prest).strip()}\"\n",
        "            if lookup_key in self.traitement_lookup:\n",
        "                result = self.traitement_lookup[lookup_key]\n",
        "                return result['code'], result['traitement']\n",
        "\n",
        "        if dechets_agrege in self.traitement_lookup:\n",
        "            result = self.traitement_lookup[dechets_agrege]\n",
        "            return result['code'], result['traitement']\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def map_site(self, site_name):\n",
        "        \"\"\"Map site name to Urbyn site info\"\"\"\n",
        "        if pd.isna(site_name):\n",
        "            return None\n",
        "\n",
        "        key = str(site_name).strip().lower()\n",
        "\n",
        "        if key in self.site_lookup:\n",
        "            return self.site_lookup[key]\n",
        "\n",
        "        for lookup_key, value in self.site_lookup.items():\n",
        "            if key in lookup_key or lookup_key in key:\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "def detect_header_row(filepath):\n",
        "    \"\"\"Detect where the actual header row is in Les Alchimistes files\"\"\"\n",
        "    df_raw = pd.read_excel(filepath, sheet_name=0, header=None, nrows=20)\n",
        "\n",
        "    for i, row in df_raw.iterrows():\n",
        "        row_values = [str(v).lower() for v in row.values if pd.notna(v)]\n",
        "        if any('date' in v for v in row_values) and any('poids' in v or 'kg' in v for v in row_values):\n",
        "            return i\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "def read_inputs(input_files):\n",
        "    \"\"\"Read all Les Alchimistes input files\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING INPUT FILES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    dfs = []\n",
        "    for name, filepath in input_files.items():\n",
        "        if os.path.exists(filepath):\n",
        "            header_row = detect_header_row(filepath)\n",
        "            df = pd.read_excel(filepath, sheet_name=0, header=header_row)\n",
        "\n",
        "            # Find date column and filter\n",
        "            date_col = None\n",
        "            for col in df.columns:\n",
        "                if 'date' in str(col).lower():\n",
        "                    date_col = col\n",
        "                    break\n",
        "\n",
        "            if date_col:\n",
        "                df = df[df[date_col].notna()].reset_index(drop=True)\n",
        "\n",
        "            df['_source_file'] = name\n",
        "            print(f\"✓ {name}: {len(df)} rows (header at row {header_row})\")\n",
        "            dfs.append(df)\n",
        "        else:\n",
        "            print(f\"⚠ File not found: {filepath}\")\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"No input files found!\")\n",
        "\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "\n",
        "def read_template(template_file):\n",
        "    \"\"\"Read template for column structure\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING TEMPLATE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df = pd.read_excel(template_file, sheet_name=0, header=TEMPLATE_HEADER_ROW)\n",
        "    print(f\"✓ Template: {len(df.columns)} columns\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def find_column(df, possible_names):\n",
        "    \"\"\"Find column by trying multiple possible names\"\"\"\n",
        "    for name in possible_names:\n",
        "        for col in df.columns:\n",
        "            if name.lower() in str(col).lower():\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "\n",
        "def transform(df_input, df_template, mapper):\n",
        "    \"\"\"Transform input data to output format\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRANSFORMING DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Find columns dynamically\n",
        "    date_col = find_column(df_input, ['date'])\n",
        "    site_col = find_column(df_input, ['site', 'client', 'établissement', 'etablissement'])\n",
        "    weight_col = find_column(df_input, ['poids', 'kg', 'masse', 'quantité'])\n",
        "    waste_col = find_column(df_input, ['matière', 'déchet', 'type'])\n",
        "    code_col = find_column(df_input, ['code', 'traitement', 'r/d'])\n",
        "\n",
        "    print(f\"  Date column: {date_col}\")\n",
        "    print(f\"  Site column: {site_col}\")\n",
        "    print(f\"  Weight column: {weight_col}\")\n",
        "    print(f\"  Waste column: {waste_col}\")\n",
        "\n",
        "    output_rows = []\n",
        "\n",
        "    for idx, row in df_input.iterrows():\n",
        "        # === SITE MAPPING ===\n",
        "        site_name = row.get(site_col, '') if site_col else ''\n",
        "        site_info = mapper.map_site(site_name)\n",
        "\n",
        "        if site_info is None:\n",
        "            # Skip rows without site mapping\n",
        "            continue\n",
        "\n",
        "        # === WASTE MAPPING ===\n",
        "        dechets_prest = row.get(waste_col, '') if waste_col else ''\n",
        "        dechet_fin = mapper.map_dechet_prestataire_to_fin(dechets_prest)\n",
        "        dechets_agrege = mapper.map_dechet_fin_to_agrege(dechet_fin)\n",
        "\n",
        "        # === TREATMENT MAPPING ===\n",
        "        code_prest = row.get(code_col, '') if code_col else ''\n",
        "        code_final, traitement = mapper.map_traitement(dechets_agrege, code_prest)\n",
        "\n",
        "        # === WEIGHT ===\n",
        "        masse_kg = row.get(weight_col, 0) if weight_col else 0\n",
        "        if pd.isna(masse_kg):\n",
        "            masse_kg = 0\n",
        "\n",
        "        # Get prestataire from site mapping or source file\n",
        "        prestataire = site_info.get('prestataire', row.get('_source_file', GROUPE_PRESTATAIRE))\n",
        "\n",
        "        # === BUILD OUTPUT ROW ===\n",
        "        new_row = {\n",
        "            'Libellé': None,\n",
        "            'Groupe': GROUPE,\n",
        "            'Code site': site_info['nom_site'].split(' - ')[0] if site_info and pd.notna(site_info.get('nom_site')) and ' - ' in str(site_info.get('nom_site', '')) else None,\n",
        "            'Nom du site': site_info['nom_site'] if site_info else 'Site inconnu',\n",
        "            'Nom du client': CLIENT,\n",
        "            'Type de porteur': 'FM',\n",
        "            'Commentaire mouvement': None,\n",
        "            'Code de la prestation': site_info['code_prestation'] if site_info else None,\n",
        "            'Prestataire': prestataire,\n",
        "            'Groupe de Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Type de prestataire': TYPE_PRESTATAIRE,\n",
        "            'Périodicité': 'Jour',\n",
        "            'Date début registre': row.get(date_col) if date_col else None,\n",
        "            'Date fin registre': row.get(date_col) if date_col else None,\n",
        "            'Code déchet prestataire': '20_01_08',\n",
        "            'Déchet fin': dechet_fin,\n",
        "            'Déchets agrégé': dechets_agrege,\n",
        "            'Déchets prestataire': dechets_prest if pd.notna(dechets_prest) and str(dechets_prest).strip() else dechet_fin,\n",
        "            'Masse totale (kg)': masse_kg,\n",
        "            'Nombre de contenants': 1,\n",
        "            'Volume contenant (L)': None,\n",
        "            'Type de contenant': None,\n",
        "            'Volume total (L)': None,\n",
        "            'Nature de quantités collectées': 'Masse',\n",
        "            'Qualité quantités': 'Document prestataire',\n",
        "            'Précision estimations des quantités': None,\n",
        "            'Traitement': traitement,\n",
        "            'Traitement prestataire': None,\n",
        "            'Code traitement': code_final if code_final else code_prest,\n",
        "            'Code traitement prestataire': code_prest if pd.notna(code_prest) else code_final,\n",
        "            'Qualité du Traitement': 'Document prestataire',\n",
        "            'N° de BSD/BSDD': None,\n",
        "            'N° de recépissé': None,\n",
        "            'Transporteur': None,\n",
        "            'Transporteur prestataire': prestataire,\n",
        "            \"Plaque d'immatriculation\": None,\n",
        "            'Exutoire intermédiaire': None,\n",
        "            'Exutoire intermédiaire prestataire': None,\n",
        "            \"Qualité de l'exutoire intermédiaire\": None,\n",
        "            'Exutoire final': None,\n",
        "            'Exutoire final prestataire': None,\n",
        "            \"Qualité de l'exutoire final\": 'Document prestataire',\n",
        "            'Période de clôture': None,\n",
        "            'Statut du mouvement': 'Réalisée',\n",
        "            'Commentaire': None,\n",
        "        }\n",
        "\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    df_output = pd.DataFrame(output_rows)\n",
        "\n",
        "    for col in df_template.columns:\n",
        "        if col not in df_output.columns:\n",
        "            df_output[col] = None\n",
        "\n",
        "    df_output = df_output[df_template.columns]\n",
        "\n",
        "    print(f\"✓ Transformed {len(df_output)} rows\")\n",
        "    return df_output\n",
        "\n",
        "\n",
        "def save_output(df_output, output_file):\n",
        "    \"\"\"Save output file\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING OUTPUT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df_output.to_excel(output_file, sheet_name=OUTPUT_SHEET, index=False)\n",
        "    print(f\"✓ Saved: {output_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LES ALCHIMISTES ETL TRANSFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        mapper = ETLMapper(ETL_FILE, PRESTATAIRE_PATTERN)\n",
        "        df_input = read_inputs(INPUT_FILES)\n",
        "        df_template = read_template(TEMPLATE_FILE)\n",
        "        df_output = transform(df_input, df_template, mapper)\n",
        "        save_output(df_output, OUTPUT_FILE)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✓✓✓ SUCCESS ✓✓✓\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf4RbXRMP0zz",
        "outputId": "777b672a-13c3-469f-fc77-d2719a634da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LES ALCHIMISTES ETL TRANSFORMATION\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING ETL MAPPINGS FOR: alchimiste\n",
            "================================================================================\n",
            "✓ Déchet sheet: 28 mappings for 'alchimiste'\n",
            "  - 4 explicit mappings\n",
            "  - Default (for empty): Biodéchets\n",
            "✓ Paramètres sheet: 121 Déchet fin → Déchets agrégé mappings\n",
            "✓ Traitement générique: 169 treatment mappings\n",
            "✓ Site sheet: 91 sites for 'alchimiste'\n",
            "\n",
            "================================================================================\n",
            "READING INPUT FILES\n",
            "================================================================================\n",
            "✓ Lesalchimistes - Idf: 76 rows (header at row 0)\n",
            "✓ Lesalchimistes - Lille: 5 rows (header at row 0)\n",
            "✓ Lesalchimistes - Languedoc - Montpellier: 4 rows (header at row 0)\n",
            "\n",
            "================================================================================\n",
            "READING TEMPLATE\n",
            "================================================================================\n",
            "✓ Template: 64 columns\n",
            "\n",
            "================================================================================\n",
            "TRANSFORMING DATA\n",
            "================================================================================\n",
            "  Date column: Date\n",
            "  Site column: Nom du client\n",
            "  Weight column: Poids net (contenu déchets)\n",
            "  Waste column: Type de matière\n",
            "✓ Transformed 9 rows\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "✓ Saved: /content/LesAlchimistes_Registre_Agrege_OUTPUT.xlsx\n",
            "\n",
            "================================================================================\n",
            "✓✓✓ SUCCESS ✓✓✓\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Paprec\n"
      ],
      "metadata": {
        "id": "VsORv8gyFu9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Paprec ETL Transformation Script (Google Colab Version)\n",
        "Transforms Paprec waste registry data to Urbyn aggregated format.\n",
        "\n",
        "Key differences from other prestataires:\n",
        "- Site identified by 'Numéro Contrat' (not address)\n",
        "- Waste type from 'Libellé Qualité'\n",
        "- Container type from 'Libellé matériel'\n",
        "- No treatment code in input (derived from ETL based on waste type)\n",
        "- Weight in 'Poids' (kg)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "INPUT_FILE = '/content/TABLE_17-12-2025_16H49.xlsx'\n",
        "TEMPLATE_FILE = '/content/Modèle vierge de Registre des déchets et Reporting des coûts SLIMAN.xlsx'\n",
        "ETL_FILE = '/content/ETL _ Mapping registre déchets prestataire vers Urbyn.xlsx'\n",
        "OUTPUT_FILE = '/content/Paprec_Registre_Agrege_OUTPUT.xlsx'\n",
        "\n",
        "PRESTATAIRE_PATTERN = 'Paprec'\n",
        "TEMPLATE_HEADER_ROW = 8\n",
        "OUTPUT_SHEET = 'Registre des déchets (Mouvement'\n",
        "\n",
        "CLIENT = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE_PRESTATAIRE = \"Paprec\"\n",
        "TYPE_PRESTATAIRE = \"Privé\"\n",
        "\n",
        "# Column patterns for dynamic detection\n",
        "COLUMN_PATTERNS = {\n",
        "    'site_id': ['numéro contrat', 'contrat', 'numero contrat', 'n° contrat'],\n",
        "    'site_ref': ['référence', 'reference', 'adresse'],\n",
        "    'date': ['date prestation', 'date', 'date collecte'],\n",
        "    'waste': ['libellé qualité', 'qualité', 'matière', 'déchet'],\n",
        "    'container': ['libellé matériel', 'matériel', 'contenant', 'bac'],\n",
        "    'quantity': ['quantité', 'nombre', 'nb'],\n",
        "    'weight': ['poids', 'masse', 'kg'],\n",
        "    'bsd': ['numerobe', 'n° be', 'bon enlèvement', 'bsd'],\n",
        "    'month': ['mois'],\n",
        "    'year': ['année', 'annee'],\n",
        "}\n",
        "\n",
        "# Container volume mapping\n",
        "CONTAINER_VOLUMES = {\n",
        "    'bac roulant 660l': 660,\n",
        "    'bac roulant 340l': 340,\n",
        "    'bac roulant 770l': 770,\n",
        "    'bac roulant 240l': 240,\n",
        "    'bac roulant 120l': 120,\n",
        "    'caisse palette 600l': 600,\n",
        "    'palette': 1000,\n",
        "    'balle': 1000,\n",
        "    'box': 1000,\n",
        "    'sac': 110,\n",
        "    'vrac': None,\n",
        "    \"croqu' feuilles\": None,\n",
        "}\n",
        "\n",
        "# Container type names\n",
        "CONTAINER_TYPES = {\n",
        "    'bac roulant 660l': 'Bac 4 roues - 660 L',\n",
        "    'bac roulant 340l': 'Bac 2 roues - 340 L',\n",
        "    'bac roulant 770l': 'Bac 4 roues - 770 L',\n",
        "    'bac roulant 240l': 'Bac 2 roues - 240 L',\n",
        "    'bac roulant 120l': 'Bac 2 roues - 120 L',\n",
        "    'caisse palette 600l': 'Caisse Palette - 600 L',\n",
        "    'palette': 'Palette - 1 m3',\n",
        "    'balle': 'Balle - 1 m3',\n",
        "    'box': 'Box - 1 m3',\n",
        "    'sac': 'Sac - 110 L',\n",
        "    'vrac': 'Equipement inconnu',\n",
        "    \"croqu' feuilles\": 'Equipement inconnu',\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# =============================================================================\n",
        "def find_column(df, possible_names):\n",
        "    for name in possible_names:\n",
        "        for col in df.columns:\n",
        "            if name.lower() in str(col).lower():\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "def safe_get(row, column, default=None):\n",
        "    if column is None:\n",
        "        return default\n",
        "    try:\n",
        "        value = row.get(column)\n",
        "        return default if pd.isna(value) else value\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def extract_code_site(nom_site):\n",
        "    if pd.isna(nom_site):\n",
        "        return None\n",
        "    nom_str = str(nom_site)\n",
        "    return nom_str.split(' - ')[0] if ' - ' in nom_str else None\n",
        "\n",
        "def get_container_volume(container_name):\n",
        "    if pd.isna(container_name):\n",
        "        return None\n",
        "    key = str(container_name).lower().strip()\n",
        "    return CONTAINER_VOLUMES.get(key)\n",
        "\n",
        "def get_container_type(container_name):\n",
        "    if pd.isna(container_name):\n",
        "        return 'Equipement inconnu'\n",
        "    key = str(container_name).lower().strip()\n",
        "    return CONTAINER_TYPES.get(key, 'Equipement inconnu')\n",
        "\n",
        "def get_periode_cloture(row, cols):\n",
        "    \"\"\"Build période de clôture from month and year\"\"\"\n",
        "    mois = safe_get(row, cols.get('month'))\n",
        "    annee = safe_get(row, cols.get('year'))\n",
        "    if mois and annee:\n",
        "        return f\"{int(mois)}_{int(annee)}\"\n",
        "    return None\n",
        "\n",
        "# =============================================================================\n",
        "# ETL MAPPER CLASS\n",
        "# =============================================================================\n",
        "class ETLMapper:\n",
        "    def __init__(self, etl_file, prestataire_pattern):\n",
        "        self.etl_file = etl_file\n",
        "        self.prestataire_pattern = prestataire_pattern\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"LOADING ETL MAPPINGS FOR: {prestataire_pattern}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.load_dechet_mapping()\n",
        "        self.load_dechet_to_agrege_mapping()\n",
        "        self.load_traitement_mapping()\n",
        "        self.load_site_mapping()\n",
        "\n",
        "    def load_dechet_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Déchet')\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(self.prestataire_pattern, case=False, na=False)\n",
        "        df_filtered = df[mask]\n",
        "        print(f\"✓ Déchet: {len(df_filtered)} mappings\")\n",
        "\n",
        "        self.dechet_lookup = {}\n",
        "        self.default_dechet_fin = None\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            prest = row.get('Nom des déchets prestataire')\n",
        "            urbyn = row.get('Nom des déchets Urbyn')\n",
        "            if pd.isna(prest):\n",
        "                if pd.notna(urbyn): self.default_dechet_fin = urbyn\n",
        "            elif pd.notna(urbyn):\n",
        "                self.dechet_lookup[str(prest).strip().lower()] = urbyn\n",
        "\n",
        "    def load_dechet_to_agrege_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Paramètres')\n",
        "        self.dechet_to_agrege = {}\n",
        "        for _, row in df[['Category', 'Name']].dropna().iterrows():\n",
        "            self.dechet_to_agrege[str(row['Name']).strip().lower()] = str(row['Category']).strip()\n",
        "        print(f\"✓ Paramètres: {len(self.dechet_to_agrege)} mappings\")\n",
        "\n",
        "    def load_traitement_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Traitement générique')\n",
        "        self.traitement_lookup = {}\n",
        "        for _, row in df.iterrows():\n",
        "            key = str(row.get('Concatener déchet & code de traitement prestataire', '')).strip()\n",
        "            if key:\n",
        "                self.traitement_lookup[key] = {\n",
        "                    'code': row.get('Code traitement retraité'),\n",
        "                    'traitement': row.get('Traitement')\n",
        "                }\n",
        "        print(f\"✓ Traitement: {len(self.traitement_lookup)} mappings\")\n",
        "\n",
        "    def load_site_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Site')\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(self.prestataire_pattern, case=False, na=False)\n",
        "        df_filtered = df[mask]\n",
        "        print(f\"✓ Site: {len(df_filtered)} mappings\")\n",
        "\n",
        "        self.site_lookup = {}\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            site_prest = row.get('Nom site prestataire')\n",
        "            if pd.notna(site_prest):\n",
        "                # Store both string and numeric versions of the key\n",
        "                key = str(site_prest).strip().lower()\n",
        "                self.site_lookup[key] = {\n",
        "                    'nom_site': row.get('Nom site Urbyn'),\n",
        "                    'code_prestation': row.get('Code de la prestation'),\n",
        "                    'prestataire': row.get('Nom prestataire (FORMULE)')\n",
        "                }\n",
        "\n",
        "    def map_dechet(self, dechet_prest):\n",
        "        if pd.isna(dechet_prest) or str(dechet_prest).strip() == '':\n",
        "            return self.default_dechet_fin\n",
        "        key = str(dechet_prest).strip().lower()\n",
        "        return self.dechet_lookup.get(key, self.default_dechet_fin)\n",
        "\n",
        "    def map_agrege(self, dechet_fin):\n",
        "        if pd.isna(dechet_fin): return None\n",
        "        return self.dechet_to_agrege.get(str(dechet_fin).strip().lower())\n",
        "\n",
        "    def map_traitement(self, agrege, code=None):\n",
        "        \"\"\"Map treatment - Paprec often has no code, so use default for waste type\"\"\"\n",
        "        if pd.isna(agrege): return None, None\n",
        "\n",
        "        # Try with code if provided\n",
        "        if pd.notna(code) and str(code).strip():\n",
        "            key = f\"{agrege}{str(code).strip()}\"\n",
        "            if key in self.traitement_lookup:\n",
        "                r = self.traitement_lookup[key]\n",
        "                return r['code'], r['traitement']\n",
        "\n",
        "        # Try default (no code)\n",
        "        if agrege in self.traitement_lookup:\n",
        "            r = self.traitement_lookup[agrege]\n",
        "            return r['code'], r['traitement']\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def map_site(self, site_id):\n",
        "        \"\"\"Map site by Numéro Contrat\"\"\"\n",
        "        if pd.isna(site_id): return None\n",
        "\n",
        "        # Try as string\n",
        "        key = str(site_id).strip().lower()\n",
        "        if key in self.site_lookup:\n",
        "            return self.site_lookup[key]\n",
        "\n",
        "        # Try as int (in case of numeric ID)\n",
        "        try:\n",
        "            key_int = str(int(float(site_id))).lower()\n",
        "            if key_int in self.site_lookup:\n",
        "                return self.site_lookup[key_int]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN FUNCTIONS\n",
        "# =============================================================================\n",
        "def read_input(input_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING INPUT FILE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        raise ValueError(f\"File not found: {input_file}\")\n",
        "\n",
        "    df = pd.read_excel(input_file, sheet_name=0, header=0)\n",
        "    print(f\"✓ Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "    # Detect columns\n",
        "    cols = {k: find_column(df, v) for k, v in COLUMN_PATTERNS.items()}\n",
        "    print(\"\\n  Detected columns:\")\n",
        "    for k, v in cols.items():\n",
        "        print(f\"    {k}: {v if v else 'NOT FOUND'}\")\n",
        "\n",
        "    return df, cols\n",
        "\n",
        "def read_template(template_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING TEMPLATE\")\n",
        "    print(\"=\"*80)\n",
        "    df = pd.read_excel(template_file, sheet_name=0, header=TEMPLATE_HEADER_ROW)\n",
        "    print(f\"✓ Template: {len(df.columns)} columns\")\n",
        "    return df\n",
        "\n",
        "def transform(df_input, df_template, mapper, cols):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRANSFORMING DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    output_rows = []\n",
        "    skipped = 0\n",
        "\n",
        "    for _, row in df_input.iterrows():\n",
        "        # Site mapping (by Numéro Contrat)\n",
        "        site_id = safe_get(row, cols.get('site_id'), '')\n",
        "        site_info = mapper.map_site(site_id)\n",
        "\n",
        "        if site_info is None:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Waste mapping (Libellé Qualité → Déchet fin → Déchets agrégé)\n",
        "        dechets_prest = safe_get(row, cols.get('waste'), '')\n",
        "        dechet_fin = mapper.map_dechet(dechets_prest)\n",
        "        dechets_agrege = mapper.map_agrege(dechet_fin)\n",
        "\n",
        "        # Treatment mapping (no code in input, derive from waste type)\n",
        "        code_final, traitement = mapper.map_traitement(dechets_agrege, None)\n",
        "\n",
        "        # Weight (already in kg)\n",
        "        masse_kg = safe_get(row, cols.get('weight'), 0)\n",
        "        if masse_kg: masse_kg = float(masse_kg)\n",
        "\n",
        "        # Container info\n",
        "        container_name = safe_get(row, cols.get('container'), '')\n",
        "        nb_contenants = safe_get(row, cols.get('quantity'), 0)\n",
        "        volume_contenant = get_container_volume(container_name)\n",
        "        type_contenant = get_container_type(container_name)\n",
        "        volume_total = volume_contenant * nb_contenants if volume_contenant and nb_contenants else 0\n",
        "\n",
        "        # Date\n",
        "        date_val = safe_get(row, cols.get('date'))\n",
        "\n",
        "        # Période de clôture\n",
        "        periode = get_periode_cloture(row, cols)\n",
        "\n",
        "        # BSD number\n",
        "        bsd = safe_get(row, cols.get('bsd'))\n",
        "\n",
        "        new_row = {\n",
        "            'Libellé': None,\n",
        "            'Groupe': GROUPE,\n",
        "            'Code site': extract_code_site(site_info['nom_site']) if site_info else None,\n",
        "            'Nom du site': site_info['nom_site'] if site_info else 'Site inconnu',\n",
        "            'Nom du client': CLIENT,\n",
        "            'Type de porteur': 'FM',\n",
        "            'Commentaire mouvement': None,\n",
        "            'Code de la prestation': site_info['code_prestation'] if site_info else None,\n",
        "            'Prestataire': site_info['prestataire'] if site_info else GROUPE_PRESTATAIRE,\n",
        "            'Groupe de Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Type de prestataire': TYPE_PRESTATAIRE,\n",
        "            'Périodicité': 'Jour',\n",
        "            'Date début registre': date_val,\n",
        "            'Date fin registre': date_val,\n",
        "            'Code déchet prestataire': None,\n",
        "            'Déchet fin': dechet_fin,\n",
        "            'Déchets agrégé': dechets_agrege,\n",
        "            'Déchets prestataire': dechets_prest if dechets_prest else dechet_fin,\n",
        "            'Masse totale (kg)': masse_kg,\n",
        "            'Nombre de contenants': nb_contenants,\n",
        "            'Volume contenant (L)': volume_contenant,\n",
        "            'Type de contenant': type_contenant,\n",
        "            'Volume total (L)': volume_total,\n",
        "            'Nature de quantités collectées': 'Volume et Masse',\n",
        "            'Qualité quantités': 'Document prestataire',\n",
        "            'Précision estimations des quantités': None,\n",
        "            'Traitement': traitement,\n",
        "            'Traitement prestataire': None,\n",
        "            'Code traitement': code_final,\n",
        "            'Code traitement prestataire': code_final,\n",
        "            'Qualité du Traitement': 'Document prestataire',\n",
        "            'N° de BSD/BSDD': bsd,\n",
        "            'N° de recépissé': None,\n",
        "            'Transporteur': 'Transporteur inconnu',\n",
        "            'Transporteur prestataire': None,\n",
        "            \"Plaque d'immatriculation\": None,\n",
        "            'Exutoire intermédiaire': 'Exutoire intermédiaire inconnu',\n",
        "            'Exutoire intermédiaire prestataire': None,\n",
        "            \"Qualité de l'exutoire intermédiaire\": 'Inconnu',\n",
        "            'Exutoire final': 'Exutoire final inconnu',\n",
        "            'Exutoire final prestataire': None,\n",
        "            \"Qualité de l'exutoire final\": 'Inconnu',\n",
        "            'Période de clôture': periode,\n",
        "            'Statut du mouvement': 'Réalisée',\n",
        "            'Commentaire': None,\n",
        "        }\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    df_output = pd.DataFrame(output_rows)\n",
        "    for col in df_template.columns:\n",
        "        if col not in df_output.columns:\n",
        "            df_output[col] = None\n",
        "    df_output = df_output[df_template.columns]\n",
        "\n",
        "    print(f\"✓ Transformed {len(df_output)} rows\")\n",
        "    if skipped:\n",
        "        print(f\"  ⚠ Skipped {skipped} rows (no site mapping)\")\n",
        "    return df_output\n",
        "\n",
        "def save_output(df_output, output_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING OUTPUT\")\n",
        "    print(\"=\"*80)\n",
        "    df_output.to_excel(output_file, sheet_name=OUTPUT_SHEET, index=False)\n",
        "    print(f\"✓ Saved: {output_file}\")\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PAPREC ETL TRANSFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        mapper = ETLMapper(ETL_FILE, PRESTATAIRE_PATTERN)\n",
        "        df_input, cols = read_input(INPUT_FILE)\n",
        "        df_template = read_template(TEMPLATE_FILE)\n",
        "        df_output = transform(df_input, df_template, mapper, cols)\n",
        "        save_output(df_output, OUTPUT_FILE)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✓✓✓ SUCCESS ✓✓✓\")\n",
        "        print(\"=\"*80)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFnNYVkiFxBI",
        "outputId": "b1b78bc8-f701-499e-8e85-cc3f3a0b58a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PAPREC ETL TRANSFORMATION\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING ETL MAPPINGS FOR: Paprec\n",
            "================================================================================\n",
            "✓ Déchet: 365 mappings\n",
            "✓ Paramètres: 121 mappings\n",
            "✓ Traitement: 169 mappings\n",
            "✓ Site: 86 mappings\n",
            "\n",
            "================================================================================\n",
            "READING INPUT FILE\n",
            "================================================================================\n",
            "✓ Loaded: 127 rows, 10 columns\n",
            "\n",
            "  Detected columns:\n",
            "    site_id: Numéro Contrat\n",
            "    site_ref: Référence\n",
            "    date: Date Prestation\n",
            "    waste: Libellé Qualité\n",
            "    container: Libellé matériel\n",
            "    quantity: Quantité\n",
            "    weight: Poids\n",
            "    bsd: NumeroBE\n",
            "    month: Mois\n",
            "    year: Année\n",
            "\n",
            "================================================================================\n",
            "READING TEMPLATE\n",
            "================================================================================\n",
            "✓ Template: 64 columns\n",
            "\n",
            "================================================================================\n",
            "TRANSFORMING DATA\n",
            "================================================================================\n",
            "✓ Transformed 119 rows\n",
            "  ⚠ Skipped 8 rows (no site mapping)\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "✓ Saved: /content/Paprec_Registre_Agrege_OUTPUT.xlsx\n",
            "\n",
            "================================================================================\n",
            "✓✓✓ SUCCESS ✓✓✓\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Elise"
      ],
      "metadata": {
        "id": "U5Nm3Wl6Iao2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Elise ETL Transformation Script (Google Colab Version)\n",
        "Transforms Elise waste registry data (CSV format) to Urbyn aggregated format.\n",
        "\n",
        "Key characteristics:\n",
        "- Input is CSV with semicolon separator\n",
        "- Site identified by 'NOM' column (site name)\n",
        "- Waste type from 'Gisement' column\n",
        "- Weight in 'Quantité (kg)' - already in kg\n",
        "- Treatment code from 'Code D/R' (intermediate) and 'Code D/R.1' (final)\n",
        "- Exutoire from 'Installation' column\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "INPUT_FILE = '/content/Reporting_Elise_Capgemini_02_25.csv'\n",
        "TEMPLATE_FILE = '/content/Modèle vierge de Registre des déchets et Reporting des coûts SLIMAN.xlsx'\n",
        "ETL_FILE = '/content/ETL _ Mapping registre déchets prestataire vers Urbyn.xlsx'\n",
        "OUTPUT_FILE = '/content/Elise_Registre_Agrege_OUTPUT.xlsx'\n",
        "\n",
        "PRESTATAIRE_PATTERN = 'Elise'\n",
        "TEMPLATE_HEADER_ROW = 8\n",
        "OUTPUT_SHEET = 'Registre des déchets (Mouvement'\n",
        "\n",
        "CLIENT = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE = \"Capgemini\"\n",
        "GROUPE_PRESTATAIRE = \"Elise\"\n",
        "TYPE_PRESTATAIRE = \"Privé\"\n",
        "\n",
        "# Column patterns for dynamic detection\n",
        "COLUMN_PATTERNS = {\n",
        "    'site_name': ['nom', 'site', 'client'],\n",
        "    'site_address': ['n° + rue', 'rue', 'adresse'],\n",
        "    'site_cp': ['cp'],\n",
        "    'site_ville': ['ville'],\n",
        "    'date': ['date collecte', 'date'],\n",
        "    'waste': ['gisement', 'matière', 'déchet', 'qualité'],\n",
        "    'waste_code': ['code nomenclature', 'nomenclature', 'code déchet'],\n",
        "    'weight': ['quantité', 'kg', 'poids', 'masse'],\n",
        "    'collector_site': ['site'],  # Collector site name\n",
        "    'treatment_code_intermediate': ['code d/r'],\n",
        "    'treatment_code_final': ['code d/r.1'],\n",
        "    'exutoire': ['installation', 'exutoire'],\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# =============================================================================\n",
        "def find_column(df, possible_names, exclude_cols=None):\n",
        "    \"\"\"Find column by keyword, optionally excluding already-found columns\"\"\"\n",
        "    exclude_cols = exclude_cols or []\n",
        "    for name in possible_names:\n",
        "        for col in df.columns:\n",
        "            if col in exclude_cols:\n",
        "                continue\n",
        "            if name.lower() in str(col).lower():\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "def safe_get(row, column, default=None):\n",
        "    if column is None:\n",
        "        return default\n",
        "    try:\n",
        "        value = row.get(column)\n",
        "        return default if pd.isna(value) else value\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def extract_code_site(nom_site):\n",
        "    if pd.isna(nom_site):\n",
        "        return None\n",
        "    nom_str = str(nom_site)\n",
        "    return nom_str.split(' - ')[0] if ' - ' in nom_str else None\n",
        "\n",
        "def detect_csv_separator(filepath):\n",
        "    \"\"\"Detect CSV separator by reading first line\"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8-sig') as f:\n",
        "        first_line = f.readline()\n",
        "\n",
        "    # Count potential separators\n",
        "    separators = {';': first_line.count(';'), ',': first_line.count(','), '\\t': first_line.count('\\t')}\n",
        "    return max(separators, key=separators.get)\n",
        "\n",
        "def detect_header_row(filepath, sep):\n",
        "    \"\"\"Detect header row in CSV\"\"\"\n",
        "    df_raw = pd.read_csv(filepath, sep=sep, header=None, nrows=10, encoding='utf-8-sig')\n",
        "    for i, row in df_raw.iterrows():\n",
        "        row_text = ' '.join([str(v).lower() for v in row.values if pd.notna(v)])\n",
        "        if 'date' in row_text and ('gisement' in row_text or 'quantité' in row_text or 'nom' in row_text):\n",
        "            return i\n",
        "    return 0\n",
        "\n",
        "# =============================================================================\n",
        "# ETL MAPPER CLASS\n",
        "# =============================================================================\n",
        "class ETLMapper:\n",
        "    def __init__(self, etl_file, prestataire_pattern):\n",
        "        self.etl_file = etl_file\n",
        "        self.prestataire_pattern = prestataire_pattern\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"LOADING ETL MAPPINGS FOR: {prestataire_pattern}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.load_dechet_mapping()\n",
        "        self.load_dechet_to_agrege_mapping()\n",
        "        self.load_traitement_mapping()\n",
        "        self.load_site_mapping()\n",
        "\n",
        "    def load_dechet_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Déchet')\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(self.prestataire_pattern, case=False, na=False)\n",
        "        df_filtered = df[mask]\n",
        "        print(f\"✓ Déchet: {len(df_filtered)} mappings\")\n",
        "\n",
        "        self.dechet_lookup = {}\n",
        "        self.default_dechet_fin = None\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            prest = row.get('Nom des déchets prestataire')\n",
        "            urbyn = row.get('Nom des déchets Urbyn')\n",
        "            if pd.isna(prest):\n",
        "                if pd.notna(urbyn): self.default_dechet_fin = urbyn\n",
        "            elif pd.notna(urbyn):\n",
        "                self.dechet_lookup[str(prest).strip().lower()] = urbyn\n",
        "\n",
        "    def load_dechet_to_agrege_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Paramètres')\n",
        "        self.dechet_to_agrege = {}\n",
        "        for _, row in df[['Category', 'Name']].dropna().iterrows():\n",
        "            self.dechet_to_agrege[str(row['Name']).strip().lower()] = str(row['Category']).strip()\n",
        "        print(f\"✓ Paramètres: {len(self.dechet_to_agrege)} mappings\")\n",
        "\n",
        "    def load_traitement_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Traitement générique')\n",
        "        self.traitement_lookup = {}\n",
        "        for _, row in df.iterrows():\n",
        "            key = str(row.get('Concatener déchet & code de traitement prestataire', '')).strip()\n",
        "            if key:\n",
        "                self.traitement_lookup[key] = {\n",
        "                    'code': row.get('Code traitement retraité'),\n",
        "                    'traitement': row.get('Traitement')\n",
        "                }\n",
        "        print(f\"✓ Traitement: {len(self.traitement_lookup)} mappings\")\n",
        "\n",
        "    def load_site_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Site')\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(self.prestataire_pattern, case=False, na=False)\n",
        "        df_filtered = df[mask]\n",
        "        print(f\"✓ Site: {len(df_filtered)} mappings\")\n",
        "\n",
        "        self.site_lookup = {}\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            site_prest = row.get('Nom site prestataire')\n",
        "            if pd.notna(site_prest):\n",
        "                key = str(site_prest).strip().lower()\n",
        "                self.site_lookup[key] = {\n",
        "                    'nom_site': row.get('Nom site Urbyn'),\n",
        "                    'code_prestation': row.get('Code de la prestation'),\n",
        "                    'prestataire': row.get('Nom prestataire (FORMULE)')\n",
        "                }\n",
        "\n",
        "    def map_dechet(self, dechet_prest):\n",
        "        if pd.isna(dechet_prest) or str(dechet_prest).strip() == '':\n",
        "            return self.default_dechet_fin\n",
        "        key = str(dechet_prest).strip().lower()\n",
        "        return self.dechet_lookup.get(key, self.default_dechet_fin)\n",
        "\n",
        "    def map_agrege(self, dechet_fin):\n",
        "        if pd.isna(dechet_fin): return None\n",
        "        return self.dechet_to_agrege.get(str(dechet_fin).strip().lower())\n",
        "\n",
        "    def map_traitement(self, agrege, code=None):\n",
        "        if pd.isna(agrege): return None, None\n",
        "\n",
        "        if pd.notna(code) and str(code).strip():\n",
        "            key = f\"{agrege}{str(code).strip()}\"\n",
        "            if key in self.traitement_lookup:\n",
        "                r = self.traitement_lookup[key]\n",
        "                return r['code'], r['traitement']\n",
        "\n",
        "        if agrege in self.traitement_lookup:\n",
        "            r = self.traitement_lookup[agrege]\n",
        "            return r['code'], r['traitement']\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def map_site(self, site_name):\n",
        "        if pd.isna(site_name): return None\n",
        "\n",
        "        key = str(site_name).strip().lower()\n",
        "\n",
        "        # Exact match\n",
        "        if key in self.site_lookup:\n",
        "            return self.site_lookup[key]\n",
        "\n",
        "        # Partial match\n",
        "        for lookup_key, value in self.site_lookup.items():\n",
        "            if key in lookup_key or lookup_key in key:\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN FUNCTIONS\n",
        "# =============================================================================\n",
        "def read_input(input_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING INPUT FILE (CSV)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        raise ValueError(f\"File not found: {input_file}\")\n",
        "\n",
        "    # Detect separator\n",
        "    sep = detect_csv_separator(input_file)\n",
        "    print(f\"  Detected separator: '{sep}'\")\n",
        "\n",
        "    # Detect header row\n",
        "    header_row = detect_header_row(input_file, sep)\n",
        "    print(f\"  Header row: {header_row}\")\n",
        "\n",
        "    # Read CSV\n",
        "    df = pd.read_csv(input_file, sep=sep, header=header_row, encoding='utf-8-sig')\n",
        "    print(f\"✓ Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "    # Detect columns\n",
        "    cols = {}\n",
        "    found_cols = []\n",
        "\n",
        "    # Find site name first\n",
        "    cols['site_name'] = find_column(df, COLUMN_PATTERNS['site_name'])\n",
        "    if cols['site_name']:\n",
        "        found_cols.append(cols['site_name'])\n",
        "\n",
        "    # Find other columns\n",
        "    for key, patterns in COLUMN_PATTERNS.items():\n",
        "        if key == 'site_name':\n",
        "            continue\n",
        "        cols[key] = find_column(df, patterns, exclude_cols=found_cols)\n",
        "        if cols[key]:\n",
        "            found_cols.append(cols[key])\n",
        "\n",
        "    # Handle duplicate column names (Code D/R appears twice)\n",
        "    # Find the second Code D/R for final treatment\n",
        "    code_dr_cols = [c for c in df.columns if 'code d/r' in str(c).lower()]\n",
        "    if len(code_dr_cols) >= 2:\n",
        "        cols['treatment_code_intermediate'] = code_dr_cols[0]\n",
        "        cols['treatment_code_final'] = code_dr_cols[1]\n",
        "    elif len(code_dr_cols) == 1:\n",
        "        cols['treatment_code_intermediate'] = code_dr_cols[0]\n",
        "        cols['treatment_code_final'] = code_dr_cols[0]\n",
        "\n",
        "    print(\"\\n  Detected columns:\")\n",
        "    for k, v in cols.items():\n",
        "        print(f\"    {k}: {v if v else 'NOT FOUND'}\")\n",
        "\n",
        "    return df, cols\n",
        "\n",
        "def read_template(template_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING TEMPLATE\")\n",
        "    print(\"=\"*80)\n",
        "    df = pd.read_excel(template_file, sheet_name=0, header=TEMPLATE_HEADER_ROW)\n",
        "    print(f\"✓ Template: {len(df.columns)} columns\")\n",
        "    return df\n",
        "\n",
        "def transform(df_input, df_template, mapper, cols):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRANSFORMING DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    output_rows = []\n",
        "    skipped = 0\n",
        "\n",
        "    for _, row in df_input.iterrows():\n",
        "        # Site mapping (by NOM)\n",
        "        site_name = safe_get(row, cols.get('site_name'), '')\n",
        "        site_info = mapper.map_site(site_name)\n",
        "\n",
        "        if site_info is None:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Waste mapping (Gisement → Déchet fin → Déchets agrégé)\n",
        "        dechets_prest = safe_get(row, cols.get('waste'), '')\n",
        "        dechet_fin = mapper.map_dechet(dechets_prest)\n",
        "        dechets_agrege = mapper.map_agrege(dechet_fin)\n",
        "\n",
        "        # Treatment mapping\n",
        "        code_prest = safe_get(row, cols.get('treatment_code_intermediate'), '')\n",
        "        code_final_from_input = safe_get(row, cols.get('treatment_code_final'), '')\n",
        "        code_final, traitement = mapper.map_traitement(dechets_agrege, code_prest)\n",
        "\n",
        "        # If no mapping found, use code from input\n",
        "        if code_final is None:\n",
        "            code_final = code_final_from_input\n",
        "\n",
        "        # Weight (already in kg)\n",
        "        masse_kg = safe_get(row, cols.get('weight'), 0)\n",
        "        if masse_kg:\n",
        "            masse_kg = float(masse_kg)\n",
        "\n",
        "        # Date\n",
        "        date_val = safe_get(row, cols.get('date'))\n",
        "\n",
        "        # Exutoire\n",
        "        exutoire = safe_get(row, cols.get('exutoire'))\n",
        "\n",
        "        # Waste code\n",
        "        waste_code = safe_get(row, cols.get('waste_code'))\n",
        "\n",
        "        new_row = {\n",
        "            'Libellé': None,\n",
        "            'Groupe': GROUPE,\n",
        "            'Code site': extract_code_site(site_info['nom_site']) if site_info else None,\n",
        "            'Nom du site': site_info['nom_site'] if site_info else 'Site inconnu',\n",
        "            'Nom du client': CLIENT,\n",
        "            'Type de porteur': 'FM',\n",
        "            'Commentaire mouvement': None,\n",
        "            'Code de la prestation': site_info['code_prestation'] if site_info else None,\n",
        "            'Prestataire': site_info['prestataire'] if site_info else GROUPE_PRESTATAIRE,\n",
        "            'Groupe de Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Type de prestataire': TYPE_PRESTATAIRE,\n",
        "            'Périodicité': 'Jour',\n",
        "            'Date début registre': date_val,\n",
        "            'Date fin registre': date_val,\n",
        "            'Code déchet prestataire': waste_code,\n",
        "            'Déchet fin': dechet_fin,\n",
        "            'Déchets agrégé': dechets_agrege,\n",
        "            'Déchets prestataire': dechets_prest if dechets_prest else dechet_fin,\n",
        "            'Masse totale (kg)': masse_kg,\n",
        "            'Nombre de contenants': 1,\n",
        "            'Volume contenant (L)': None,\n",
        "            'Type de contenant': None,\n",
        "            'Volume total (L)': None,\n",
        "            'Nature de quantités collectées': 'Masse',\n",
        "            'Qualité quantités': 'Document prestataire',\n",
        "            'Précision estimations des quantités': None,\n",
        "            'Traitement': traitement,\n",
        "            'Traitement prestataire': None,\n",
        "            'Code traitement': code_final,\n",
        "            'Code traitement prestataire': code_prest,\n",
        "            'Qualité du Traitement': 'Document prestataire',\n",
        "            'N° de BSD/BSDD': None,\n",
        "            'N° de recépissé': None,\n",
        "            'Transporteur': None,\n",
        "            'Transporteur prestataire': None,\n",
        "            \"Plaque d'immatriculation\": None,\n",
        "            'Exutoire intermédiaire': None,\n",
        "            'Exutoire intermédiaire prestataire': None,\n",
        "            \"Qualité de l'exutoire intermédiaire\": None,\n",
        "            'Exutoire final': exutoire,\n",
        "            'Exutoire final prestataire': exutoire,\n",
        "            \"Qualité de l'exutoire final\": 'Document prestataire',\n",
        "            'Période de clôture': None,\n",
        "            'Statut du mouvement': 'Réalisée',\n",
        "            'Commentaire': None,\n",
        "        }\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    df_output = pd.DataFrame(output_rows)\n",
        "    for col in df_template.columns:\n",
        "        if col not in df_output.columns:\n",
        "            df_output[col] = None\n",
        "    df_output = df_output[df_template.columns]\n",
        "\n",
        "    print(f\"✓ Transformed {len(df_output)} rows\")\n",
        "    if skipped:\n",
        "        print(f\"  ⚠ Skipped {skipped} rows (no site mapping)\")\n",
        "    return df_output\n",
        "\n",
        "def save_output(df_output, output_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING OUTPUT\")\n",
        "    print(\"=\"*80)\n",
        "    df_output.to_excel(output_file, sheet_name=OUTPUT_SHEET, index=False)\n",
        "    print(f\"✓ Saved: {output_file}\")\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ELISE ETL TRANSFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        mapper = ETLMapper(ETL_FILE, PRESTATAIRE_PATTERN)\n",
        "        df_input, cols = read_input(INPUT_FILE)\n",
        "        df_template = read_template(TEMPLATE_FILE)\n",
        "        df_output = transform(df_input, df_template, mapper, cols)\n",
        "        save_output(df_output, OUTPUT_FILE)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✓✓✓ SUCCESS ✓✓✓\")\n",
        "        print(\"=\"*80)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnDik1cmIcNd",
        "outputId": "d4c16ae7-d86b-4c76-de2b-c056d0a6916d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ELISE ETL TRANSFORMATION\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING ETL MAPPINGS FOR: Elise\n",
            "================================================================================\n",
            "✓ Déchet: 260 mappings\n",
            "✓ Paramètres: 121 mappings\n",
            "✓ Traitement: 169 mappings\n",
            "✓ Site: 11 mappings\n",
            "\n",
            "================================================================================\n",
            "READING INPUT FILE (CSV)\n",
            "================================================================================\n",
            "  Detected separator: ';'\n",
            "  Header row: 1\n",
            "✓ Loaded: 11 rows, 26 columns\n",
            "\n",
            "  Detected columns:\n",
            "    site_name: NOM\n",
            "    site_address: N° + Rue\n",
            "    site_cp: CP\n",
            "    site_ville: VILLE\n",
            "    date: Date Collecte\n",
            "    waste: Gisement\n",
            "    waste_code: Code Nomenclature Déchets (cf base article)\n",
            "    weight: Quantité (kg)\n",
            "    collector_site: Site\n",
            "    treatment_code_intermediate: Code D/R\n",
            "    treatment_code_final: Code D/R.1\n",
            "    exutoire: Installation\n",
            "\n",
            "================================================================================\n",
            "READING TEMPLATE\n",
            "================================================================================\n",
            "✓ Template: 64 columns\n",
            "\n",
            "================================================================================\n",
            "TRANSFORMING DATA\n",
            "================================================================================\n",
            "✓ Transformed 11 rows\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "✓ Saved: /content/Elise_Registre_Agrege_OUTPUT.xlsx\n",
            "\n",
            "================================================================================\n",
            "✓✓✓ SUCCESS ✓✓✓\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Screlec"
      ],
      "metadata": {
        "id": "hyt3ovKHKzMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Screlec ETL Transformation Script (Google Colab Version)\n",
        "Transforms Screlec BSDD (Bordereau de Suivi des Déchets Dangereux) data to Urbyn format.\n",
        "\n",
        "Key characteristics:\n",
        "- Input is BSDD format (dangerous waste tracking form)\n",
        "- Site identified by 'Expéditeur Nom usuel' (e.g., \"147\", \"FR181 AIX EN PCE\")\n",
        "- Waste type from 'Dénomination usuelle' (e.g., \"Piles en mélange\")\n",
        "- Weight in TONNES: 'Quantité réceptionnée nette (tonnes)'\n",
        "- Treatment code: 'Code opération réalisé'\n",
        "- BSD number available: 'N° de bordereau'\n",
        "- Dangerous waste code: 'Code du déchet' (e.g., \"20 01 33*\")\n",
        "\n",
        "Note: Screlec is an éco-organisme for batteries/piles collection.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "INPUT_FILE = '/content/Reporting_Screlec_Capgemini_02_25.xlsx'\n",
        "TEMPLATE_FILE = '/content/Modèle vierge de Registre des déchets et Reporting des coûts SLIMAN.xlsx'\n",
        "ETL_FILE = '/content/ETL _ Mapping registre déchets prestataire vers Urbyn.xlsx'\n",
        "OUTPUT_FILE = '/content/Screlec_Registre_Agrege_OUTPUT.xlsx'\n",
        "\n",
        "PRESTATAIRE_PATTERN = 'Screlec'\n",
        "TEMPLATE_HEADER_ROW = 8\n",
        "OUTPUT_SHEET = 'Registre des déchets (Mouvement'\n",
        "\n",
        "CLIENT = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE = \"Capgemini\"\n",
        "GROUPE_PRESTATAIRE = \"Screlec\"\n",
        "TYPE_PRESTATAIRE = \"Eco-organisme\"\n",
        "\n",
        "# Column patterns for dynamic detection\n",
        "COLUMN_PATTERNS = {\n",
        "    'bsd': ['n° de bordereau', 'bordereau', 'bsd'],\n",
        "    'date': ['date de réalisation', 'date d\\'expédition', 'date'],\n",
        "    'waste_name': ['dénomination usuelle', 'dénomination', 'déchet'],\n",
        "    'waste_code': ['code du déchet', 'code déchet', 'nomenclature'],\n",
        "    'weight': ['quantité réceptionnée', 'quantité acceptée', 'quantité', 'tonnes'],\n",
        "    'site_name': ['expéditeur nom usuel', 'nom usuel', 'expéditeur'],\n",
        "    'site_address': ['expéditeur adresse', 'adresse'],\n",
        "    'treatment_code_planned': ['code opération prévu', 'opération prévu'],\n",
        "    'treatment_code_done': ['code opération réalisé', 'opération réalisé'],\n",
        "    'transporter': ['transporteur raison sociale', 'transporteur'],\n",
        "    'destination': ['destination raison sociale', 'destination'],\n",
        "    'dangerous': ['déchet dangereux', 'dangereux'],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# =============================================================================\n",
        "def find_column(df, possible_names):\n",
        "    for name in possible_names:\n",
        "        for col in df.columns:\n",
        "            if name.lower() in str(col).lower():\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "def safe_get(row, column, default=None):\n",
        "    if column is None:\n",
        "        return default\n",
        "    try:\n",
        "        value = row.get(column)\n",
        "        return default if pd.isna(value) else value\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def extract_code_site(nom_site):\n",
        "    if pd.isna(nom_site):\n",
        "        return None\n",
        "    nom_str = str(nom_site)\n",
        "    return nom_str.split(' - ')[0] if ' - ' in nom_str else None\n",
        "\n",
        "# =============================================================================\n",
        "# ETL MAPPER CLASS\n",
        "# =============================================================================\n",
        "class ETLMapper:\n",
        "    def __init__(self, etl_file, prestataire_pattern):\n",
        "        self.etl_file = etl_file\n",
        "        self.prestataire_pattern = prestataire_pattern\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"LOADING ETL MAPPINGS FOR: {prestataire_pattern}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.load_dechet_mapping()\n",
        "        self.load_dechet_to_agrege_mapping()\n",
        "        self.load_traitement_mapping()\n",
        "        self.load_site_mapping()\n",
        "\n",
        "    def load_dechet_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Déchet')\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(self.prestataire_pattern, case=False, na=False)\n",
        "        df_filtered = df[mask]\n",
        "        print(f\"✓ Déchet: {len(df_filtered)} mappings\")\n",
        "\n",
        "        self.dechet_lookup = {}\n",
        "        self.default_dechet_fin = None\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            prest = row.get('Nom des déchets prestataire')\n",
        "            urbyn = row.get('Nom des déchets Urbyn')\n",
        "            if pd.isna(prest):\n",
        "                if pd.notna(urbyn): self.default_dechet_fin = urbyn\n",
        "            elif pd.notna(urbyn):\n",
        "                self.dechet_lookup[str(prest).strip().lower()] = urbyn\n",
        "\n",
        "    def load_dechet_to_agrege_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Paramètres')\n",
        "        self.dechet_to_agrege = {}\n",
        "        for _, row in df[['Category', 'Name']].dropna().iterrows():\n",
        "            self.dechet_to_agrege[str(row['Name']).strip().lower()] = str(row['Category']).strip()\n",
        "        print(f\"✓ Paramètres: {len(self.dechet_to_agrege)} mappings\")\n",
        "\n",
        "    def load_traitement_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Traitement générique')\n",
        "        self.traitement_lookup = {}\n",
        "        for _, row in df.iterrows():\n",
        "            key = str(row.get('Concatener déchet & code de traitement prestataire', '')).strip()\n",
        "            if key:\n",
        "                self.traitement_lookup[key] = {\n",
        "                    'code': row.get('Code traitement retraité'),\n",
        "                    'traitement': row.get('Traitement')\n",
        "                }\n",
        "        print(f\"✓ Traitement: {len(self.traitement_lookup)} mappings\")\n",
        "\n",
        "    def load_site_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Site')\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(self.prestataire_pattern, case=False, na=False)\n",
        "        df_filtered = df[mask]\n",
        "        print(f\"✓ Site: {len(df_filtered)} mappings\")\n",
        "\n",
        "        self.site_lookup = {}\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            site_prest = row.get('Nom site prestataire')\n",
        "            if pd.notna(site_prest):\n",
        "                key = str(site_prest).strip().lower()\n",
        "                self.site_lookup[key] = {\n",
        "                    'nom_site': row.get('Nom site Urbyn'),\n",
        "                    'code_prestation': row.get('Code de la prestation'),\n",
        "                    'prestataire': row.get('Nom prestataire (FORMULE)')\n",
        "                }\n",
        "\n",
        "    def map_dechet(self, dechet_prest):\n",
        "        if pd.isna(dechet_prest) or str(dechet_prest).strip() == '':\n",
        "            return self.default_dechet_fin\n",
        "        key = str(dechet_prest).strip().lower()\n",
        "        return self.dechet_lookup.get(key, self.default_dechet_fin)\n",
        "\n",
        "    def map_agrege(self, dechet_fin):\n",
        "        if pd.isna(dechet_fin):\n",
        "            return None\n",
        "        return self.dechet_to_agrege.get(str(dechet_fin).strip().lower())\n",
        "\n",
        "    def map_traitement(self, agrege, code=None):\n",
        "        if pd.isna(agrege):\n",
        "            return None, None\n",
        "\n",
        "        if pd.notna(code) and str(code).strip():\n",
        "            key = f\"{agrege}{str(code).strip()}\"\n",
        "            if key in self.traitement_lookup:\n",
        "                r = self.traitement_lookup[key]\n",
        "                return r['code'], r['traitement']\n",
        "\n",
        "        if agrege in self.traitement_lookup:\n",
        "            r = self.traitement_lookup[agrege]\n",
        "            return r['code'], r['traitement']\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def map_site(self, site_name):\n",
        "        if pd.isna(site_name):\n",
        "            return None\n",
        "\n",
        "        key = str(site_name).strip().lower()\n",
        "\n",
        "        # Exact match\n",
        "        if key in self.site_lookup:\n",
        "            return self.site_lookup[key]\n",
        "\n",
        "        # Partial match\n",
        "        for lookup_key, value in self.site_lookup.items():\n",
        "            if key in lookup_key or lookup_key in key:\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN FUNCTIONS\n",
        "# =============================================================================\n",
        "def read_input(input_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING INPUT FILE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        raise ValueError(f\"File not found: {input_file}\")\n",
        "\n",
        "    # Try to find the right sheet\n",
        "    xls = pd.ExcelFile(input_file)\n",
        "    sheet_name = 'registre' if 'registre' in [s.lower() for s in xls.sheet_names] else 0\n",
        "\n",
        "    df = pd.read_excel(input_file, sheet_name=sheet_name, header=0)\n",
        "    print(f\"✓ Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "    # Detect columns\n",
        "    cols = {k: find_column(df, v) for k, v in COLUMN_PATTERNS.items()}\n",
        "    print(\"\\n  Detected columns:\")\n",
        "    for k, v in cols.items():\n",
        "        print(f\"    {k}: {v if v else 'NOT FOUND'}\")\n",
        "\n",
        "    return df, cols\n",
        "\n",
        "def read_template(template_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING TEMPLATE\")\n",
        "    print(\"=\"*80)\n",
        "    df = pd.read_excel(template_file, sheet_name=0, header=TEMPLATE_HEADER_ROW)\n",
        "    print(f\"✓ Template: {len(df.columns)} columns\")\n",
        "    return df\n",
        "\n",
        "def transform(df_input, df_template, mapper, cols):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRANSFORMING DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    output_rows = []\n",
        "    skipped = 0\n",
        "\n",
        "    for _, row in df_input.iterrows():\n",
        "        # Site mapping\n",
        "        site_name = safe_get(row, cols.get('site_name'), '')\n",
        "        site_info = mapper.map_site(site_name)\n",
        "\n",
        "        if site_info is None:\n",
        "            skipped += 1\n",
        "            print(f\"  ⚠ No site mapping for: '{site_name}'\")\n",
        "            # Still process but mark as unknown\n",
        "            site_info = {'nom_site': 'Site inconnu', 'code_prestation': None}\n",
        "\n",
        "        # Waste mapping\n",
        "        dechets_prest = safe_get(row, cols.get('waste_name'), '')\n",
        "        dechet_fin = mapper.map_dechet(dechets_prest)\n",
        "        dechets_agrege = mapper.map_agrege(dechet_fin)\n",
        "\n",
        "        # Treatment mapping\n",
        "        code_prest = safe_get(row, cols.get('treatment_code_done'), '')\n",
        "        code_final, traitement = mapper.map_traitement(dechets_agrege, code_prest)\n",
        "\n",
        "        # If no mapping found, use code from input\n",
        "        if code_final is None:\n",
        "            code_final = code_prest\n",
        "\n",
        "        # Weight conversion (TONNES → KG)\n",
        "        poids_tonnes = safe_get(row, cols.get('weight'), 0)\n",
        "        if poids_tonnes:\n",
        "            masse_kg = float(poids_tonnes) * 1000\n",
        "        else:\n",
        "            masse_kg = 0\n",
        "\n",
        "        # Date\n",
        "        date_val = safe_get(row, cols.get('date'))\n",
        "\n",
        "        # BSD number\n",
        "        bsd = safe_get(row, cols.get('bsd'))\n",
        "\n",
        "        # Waste code (dangerous)\n",
        "        waste_code = safe_get(row, cols.get('waste_code'))\n",
        "\n",
        "        # Transporter\n",
        "        transporter = safe_get(row, cols.get('transporter'))\n",
        "\n",
        "        # Destination (exutoire)\n",
        "        exutoire = safe_get(row, cols.get('destination'))\n",
        "\n",
        "        new_row = {\n",
        "            'Libellé': None,\n",
        "            'Groupe': GROUPE,\n",
        "            'Code site': extract_code_site(site_info['nom_site']) if site_info else None,\n",
        "            'Nom du site': site_info['nom_site'] if site_info else 'Site inconnu',\n",
        "            'Nom du client': CLIENT,\n",
        "            'Type de porteur': 'FM',\n",
        "            'Commentaire mouvement': None,\n",
        "            'Code de la prestation': site_info.get('code_prestation') if site_info else None,\n",
        "            'Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Groupe de Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Type de prestataire': TYPE_PRESTATAIRE,\n",
        "            'Périodicité': 'Jour',\n",
        "            'Date début registre': date_val,\n",
        "            'Date fin registre': date_val,\n",
        "            'Code déchet prestataire': waste_code,\n",
        "            'Déchet fin': dechet_fin,\n",
        "            'Déchets agrégé': dechets_agrege,\n",
        "            'Déchets prestataire': dechets_prest if dechets_prest else dechet_fin,\n",
        "            'Masse totale (kg)': masse_kg,\n",
        "            'Nombre de contenants': 1,\n",
        "            'Volume contenant (L)': None,\n",
        "            'Type de contenant': None,\n",
        "            'Volume total (L)': None,\n",
        "            'Nature de quantités collectées': 'Masse',\n",
        "            'Qualité quantités': 'Document prestataire',\n",
        "            'Précision estimations des quantités': None,\n",
        "            'Traitement': traitement,\n",
        "            'Traitement prestataire': None,\n",
        "            'Code traitement': code_final,\n",
        "            'Code traitement prestataire': code_prest,\n",
        "            'Qualité du Traitement': 'Document prestataire',\n",
        "            'N° de BSD/BSDD': bsd,\n",
        "            'N° de recépissé': None,\n",
        "            'Transporteur': transporter,\n",
        "            'Transporteur prestataire': transporter,\n",
        "            \"Plaque d'immatriculation\": None,\n",
        "            'Exutoire intermédiaire': None,\n",
        "            'Exutoire intermédiaire prestataire': None,\n",
        "            \"Qualité de l'exutoire intermédiaire\": None,\n",
        "            'Exutoire final': exutoire,\n",
        "            'Exutoire final prestataire': exutoire,\n",
        "            \"Qualité de l'exutoire final\": 'Document prestataire',\n",
        "            'Période de clôture': None,\n",
        "            'Statut du mouvement': 'Réalisée',\n",
        "            'Commentaire': None,\n",
        "        }\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    df_output = pd.DataFrame(output_rows)\n",
        "    for col in df_template.columns:\n",
        "        if col not in df_output.columns:\n",
        "            df_output[col] = None\n",
        "    df_output = df_output[df_template.columns]\n",
        "\n",
        "    print(f\"✓ Transformed {len(df_output)} rows\")\n",
        "    if skipped:\n",
        "        print(f\"  ⚠ {skipped} rows had no site mapping (marked as 'Site inconnu')\")\n",
        "    return df_output\n",
        "\n",
        "def save_output(df_output, output_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING OUTPUT\")\n",
        "    print(\"=\"*80)\n",
        "    df_output.to_excel(output_file, sheet_name=OUTPUT_SHEET, index=False)\n",
        "    print(f\"✓ Saved: {output_file}\")\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SCRELEC ETL TRANSFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        mapper = ETLMapper(ETL_FILE, PRESTATAIRE_PATTERN)\n",
        "        df_input, cols = read_input(INPUT_FILE)\n",
        "        df_template = read_template(TEMPLATE_FILE)\n",
        "        df_output = transform(df_input, df_template, mapper, cols)\n",
        "        save_output(df_output, OUTPUT_FILE)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✓✓✓ SUCCESS ✓✓✓\")\n",
        "        print(\"=\"*80)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvcI2aECK1VK",
        "outputId": "b4012945-1d72-49c7-ceb6-d343f6e7d781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SCRELEC ETL TRANSFORMATION\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING ETL MAPPINGS FOR: Screlec\n",
            "================================================================================\n",
            "✓ Déchet: 0 mappings\n",
            "✓ Paramètres: 121 mappings\n",
            "✓ Traitement: 169 mappings\n",
            "✓ Site: 0 mappings\n",
            "\n",
            "================================================================================\n",
            "READING INPUT FILE\n",
            "================================================================================\n",
            "✓ Loaded: 3 rows, 140 columns\n",
            "\n",
            "  Detected columns:\n",
            "    bsd: N° de bordereau\n",
            "    date: Date de réalisation de l'opération\n",
            "    waste_name: Dénomination usuelle\n",
            "    waste_code: Code du déchet\n",
            "    weight: Quantité réceptionnée nette (tonnes)\n",
            "    site_name: Expéditeur Nom usuel\n",
            "    site_address: Expéditeur Adresse\n",
            "    treatment_code_planned: Code opération prévu\n",
            "    treatment_code_done: Code opération réalisé\n",
            "    transporter: Transporteur raison sociale\n",
            "    destination: Destination raison sociale\n",
            "    dangerous: Déchet dangereux\n",
            "\n",
            "================================================================================\n",
            "READING TEMPLATE\n",
            "================================================================================\n",
            "✓ Template: 64 columns\n",
            "\n",
            "================================================================================\n",
            "TRANSFORMING DATA\n",
            "================================================================================\n",
            "  ⚠ No site mapping for: '147'\n",
            "  ⚠ No site mapping for: 'FR181 AIX EN PCE'\n",
            "  ⚠ No site mapping for: '147'\n",
            "✓ Transformed 3 rows\n",
            "  ⚠ 3 rows had no site mapping (marked as 'Site inconnu')\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "✓ Saved: /content/Screlec_Registre_Agrege_OUTPUT.xlsx\n",
            "\n",
            "================================================================================\n",
            "✓✓✓ SUCCESS ✓✓✓\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#trackdechet"
      ],
      "metadata": {
        "id": "5Ktp0R0fNPI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Trackdechet ETL Transformation Script (Google Colab Version)\n",
        "Transforms Trackdechet BSDD/BSDASRI exports to Urbyn aggregated format.\n",
        "\n",
        "Trackdechet is the French government platform for tracking hazardous waste (BSD).\n",
        "This script handles exports from the platform which contain BSD data from various sources.\n",
        "\n",
        "Key characteristics:\n",
        "- Input is BSDD/BSDASRI format (142 columns)\n",
        "- Site identified by 'Expéditeur Nom usuel' (e.g., \"147\", \"AXEO\", \"LYON IVOIRE - FR178\")\n",
        "- Waste type from 'Dénomination usuelle' (e.g., \"DEEE en mélange Vrac\", \"Piles et batteries\")\n",
        "- Weight in TONNES: 'Quantité réceptionnée nette (tonnes)'\n",
        "- Treatment code: 'Code opération réalisé'\n",
        "- BSD number: 'N° de bordereau'\n",
        "- Dangerous waste code: 'Code du déchet' (e.g., \"16 02 13*\", \"20 01 33*\")\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "INPUT_FILE = '/content/TD-Registre-20250425-Sortant.xlsx'\n",
        "TEMPLATE_FILE = '/content/Modèle vierge de Registre des déchets et Reporting des coûts SLIMAN.xlsx'\n",
        "ETL_FILE = '/content/ETL _ Mapping registre déchets prestataire vers Urbyn.xlsx'\n",
        "OUTPUT_FILE = '/content/Trackdechet_Registre_Agrege_OUTPUT.xlsx'\n",
        "\n",
        "PRESTATAIRE_PATTERN = 'Trackdechet'\n",
        "TEMPLATE_HEADER_ROW = 8\n",
        "OUTPUT_SHEET = 'Registre des déchets (Mouvement'\n",
        "\n",
        "CLIENT = \"CAPGEMINI TECHNOLOGY SERVICES\"\n",
        "GROUPE = \"Capgemini\"\n",
        "GROUPE_PRESTATAIRE = \"Trackdechet\"\n",
        "TYPE_PRESTATAIRE = \"Plateforme gouvernementale\"\n",
        "\n",
        "# Column patterns for dynamic detection\n",
        "COLUMN_PATTERNS = {\n",
        "    'bsd': ['n° de bordereau', 'bordereau', 'bsd'],\n",
        "    'date': ['date de réalisation', 'date d\\'expédition', 'date'],\n",
        "    'waste_name': ['dénomination usuelle', 'dénomination', 'déchet'],\n",
        "    'waste_code': ['code du déchet', 'code déchet'],\n",
        "    'weight': ['quantité réceptionnée', 'quantité acceptée', 'quantité'],\n",
        "    'site_name': ['expéditeur nom usuel', 'nom usuel'],\n",
        "    'site_address': ['expéditeur adresse', 'adresse'],\n",
        "    'site_raison_sociale': ['expéditeur raison sociale'],\n",
        "    'treatment_code_planned': ['code opération prévu', 'opération prévu'],\n",
        "    'treatment_code_done': ['code opération réalisé', 'opération réalisé'],\n",
        "    'transporter': ['transporteur raison sociale', 'transporteur'],\n",
        "    'destination': ['destination raison sociale', 'destination'],\n",
        "    'type_bordereau': ['type de bordereau'],\n",
        "    'dangerous': ['déchet dangereux', 'dangereux'],\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# =============================================================================\n",
        "def find_column(df, possible_names):\n",
        "    for name in possible_names:\n",
        "        for col in df.columns:\n",
        "            if name.lower() in str(col).lower():\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "def safe_get(row, column, default=None):\n",
        "    if column is None:\n",
        "        return default\n",
        "    try:\n",
        "        value = row.get(column)\n",
        "        return default if pd.isna(value) else value\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def extract_code_site(nom_site):\n",
        "    if pd.isna(nom_site):\n",
        "        return None\n",
        "    nom_str = str(nom_site)\n",
        "    return nom_str.split(' - ')[0] if ' - ' in nom_str else None\n",
        "\n",
        "# =============================================================================\n",
        "# ETL MAPPER CLASS\n",
        "# =============================================================================\n",
        "class ETLMapper:\n",
        "    def __init__(self, etl_file, prestataire_pattern):\n",
        "        self.etl_file = etl_file\n",
        "        self.prestataire_pattern = prestataire_pattern\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"LOADING ETL MAPPINGS FOR: {prestataire_pattern}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.load_dechet_mapping()\n",
        "        self.load_dechet_to_agrege_mapping()\n",
        "        self.load_traitement_mapping()\n",
        "        self.load_site_mapping()\n",
        "\n",
        "    def load_dechet_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Déchet')\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(self.prestataire_pattern, case=False, na=False)\n",
        "        df_filtered = df[mask]\n",
        "        print(f\"✓ Déchet: {len(df_filtered)} mappings\")\n",
        "\n",
        "        self.dechet_lookup = {}\n",
        "        self.default_dechet_fin = None\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            prest = row.get('Nom des déchets prestataire')\n",
        "            urbyn = row.get('Nom des déchets Urbyn')\n",
        "            if pd.isna(prest):\n",
        "                if pd.notna(urbyn): self.default_dechet_fin = urbyn\n",
        "            elif pd.notna(urbyn):\n",
        "                self.dechet_lookup[str(prest).strip().lower()] = urbyn\n",
        "\n",
        "    def load_dechet_to_agrege_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Paramètres')\n",
        "        self.dechet_to_agrege = {}\n",
        "        for _, row in df[['Category', 'Name']].dropna().iterrows():\n",
        "            self.dechet_to_agrege[str(row['Name']).strip().lower()] = str(row['Category']).strip()\n",
        "        print(f\"✓ Paramètres: {len(self.dechet_to_agrege)} mappings\")\n",
        "\n",
        "    def load_traitement_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Traitement générique')\n",
        "        self.traitement_lookup = {}\n",
        "        for _, row in df.iterrows():\n",
        "            key = str(row.get('Concatener déchet & code de traitement prestataire', '')).strip()\n",
        "            if key:\n",
        "                self.traitement_lookup[key] = {\n",
        "                    'code': row.get('Code traitement retraité'),\n",
        "                    'traitement': row.get('Traitement')\n",
        "                }\n",
        "        print(f\"✓ Traitement: {len(self.traitement_lookup)} mappings\")\n",
        "\n",
        "    def load_site_mapping(self):\n",
        "        df = pd.read_excel(self.etl_file, sheet_name='Site')\n",
        "        mask = df['Nom prestataire (FORMULE)'].str.contains(self.prestataire_pattern, case=False, na=False)\n",
        "        df_filtered = df[mask]\n",
        "        print(f\"✓ Site: {len(df_filtered)} mappings\")\n",
        "\n",
        "        self.site_lookup = {}\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            site_prest = row.get('Nom site prestataire')\n",
        "            if pd.notna(site_prest):\n",
        "                key = str(site_prest).strip().lower()\n",
        "                self.site_lookup[key] = {\n",
        "                    'nom_site': row.get('Nom site Urbyn'),\n",
        "                    'code_prestation': row.get('Code de la prestation'),\n",
        "                    'prestataire': row.get('Nom prestataire (FORMULE)')\n",
        "                }\n",
        "\n",
        "    def map_dechet(self, dechet_prest):\n",
        "        if pd.isna(dechet_prest) or str(dechet_prest).strip() == '':\n",
        "            return self.default_dechet_fin\n",
        "        key = str(dechet_prest).strip().lower()\n",
        "        return self.dechet_lookup.get(key, self.default_dechet_fin)\n",
        "\n",
        "    def map_agrege(self, dechet_fin):\n",
        "        if pd.isna(dechet_fin):\n",
        "            return None\n",
        "        return self.dechet_to_agrege.get(str(dechet_fin).strip().lower())\n",
        "\n",
        "    def map_traitement(self, agrege, code=None):\n",
        "        if pd.isna(agrege):\n",
        "            return None, None\n",
        "\n",
        "        if pd.notna(code) and str(code).strip():\n",
        "            key = f\"{agrege}{str(code).strip()}\"\n",
        "            if key in self.traitement_lookup:\n",
        "                r = self.traitement_lookup[key]\n",
        "                return r['code'], r['traitement']\n",
        "\n",
        "        if agrege in self.traitement_lookup:\n",
        "            r = self.traitement_lookup[agrege]\n",
        "            return r['code'], r['traitement']\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def map_site(self, site_name):\n",
        "        if pd.isna(site_name):\n",
        "            return None\n",
        "\n",
        "        key = str(site_name).strip().lower()\n",
        "\n",
        "        # Exact match\n",
        "        if key in self.site_lookup:\n",
        "            return self.site_lookup[key]\n",
        "\n",
        "        # Partial match\n",
        "        for lookup_key, value in self.site_lookup.items():\n",
        "            if key in lookup_key or lookup_key in key:\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN FUNCTIONS\n",
        "# =============================================================================\n",
        "def read_input(input_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING INPUT FILE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        raise ValueError(f\"File not found: {input_file}\")\n",
        "\n",
        "    # Try to find the right sheet\n",
        "    xls = pd.ExcelFile(input_file)\n",
        "    sheet_name = 'registre' if 'registre' in [s.lower() for s in xls.sheet_names] else 0\n",
        "\n",
        "    df = pd.read_excel(input_file, sheet_name=sheet_name, header=0)\n",
        "    print(f\"✓ Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "    # Detect columns\n",
        "    cols = {k: find_column(df, v) for k, v in COLUMN_PATTERNS.items()}\n",
        "    print(\"\\n  Detected columns:\")\n",
        "    for k, v in cols.items():\n",
        "        print(f\"    {k}: {v if v else 'NOT FOUND'}\")\n",
        "\n",
        "    return df, cols\n",
        "\n",
        "def read_template(template_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READING TEMPLATE\")\n",
        "    print(\"=\"*80)\n",
        "    df = pd.read_excel(template_file, sheet_name=0, header=TEMPLATE_HEADER_ROW)\n",
        "    print(f\"✓ Template: {len(df.columns)} columns\")\n",
        "    return df\n",
        "\n",
        "def transform(df_input, df_template, mapper, cols):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRANSFORMING DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    output_rows = []\n",
        "    skipped_no_site = 0\n",
        "    skipped_no_weight = 0\n",
        "\n",
        "    for _, row in df_input.iterrows():\n",
        "        # Skip rows without weight (incomplete BSD)\n",
        "        weight = safe_get(row, cols.get('weight'))\n",
        "        if pd.isna(weight) or weight == 0:\n",
        "            skipped_no_weight += 1\n",
        "            continue\n",
        "\n",
        "        # Site mapping\n",
        "        site_name = safe_get(row, cols.get('site_name'), '')\n",
        "        site_info = mapper.map_site(site_name)\n",
        "\n",
        "        if site_info is None:\n",
        "            skipped_no_site += 1\n",
        "            print(f\"  ⚠ No site mapping for: '{site_name}'\")\n",
        "            # Still process but mark as unknown\n",
        "            site_info = {'nom_site': 'Site inconnu', 'code_prestation': None, 'prestataire': None}\n",
        "\n",
        "        # Waste mapping\n",
        "        dechets_prest = safe_get(row, cols.get('waste_name'), '')\n",
        "        dechet_fin = mapper.map_dechet(dechets_prest)\n",
        "        dechets_agrege = mapper.map_agrege(dechet_fin)\n",
        "\n",
        "        # Treatment mapping\n",
        "        code_prest = safe_get(row, cols.get('treatment_code_done'), '')\n",
        "        code_final, traitement = mapper.map_traitement(dechets_agrege, code_prest)\n",
        "\n",
        "        # If no mapping found, use code from input\n",
        "        if code_final is None:\n",
        "            code_final = code_prest\n",
        "\n",
        "        # Weight conversion (TONNES → KG)\n",
        "        masse_kg = float(weight) * 1000 if weight else 0\n",
        "\n",
        "        # Date - prefer date de réalisation, fallback to date d'expédition\n",
        "        date_val = safe_get(row, cols.get('date'))\n",
        "\n",
        "        # BSD number\n",
        "        bsd = safe_get(row, cols.get('bsd'))\n",
        "\n",
        "        # Waste code (dangerous)\n",
        "        waste_code = safe_get(row, cols.get('waste_code'))\n",
        "\n",
        "        # Transporter\n",
        "        transporter = safe_get(row, cols.get('transporter'))\n",
        "\n",
        "        # Destination (exutoire)\n",
        "        exutoire = safe_get(row, cols.get('destination'))\n",
        "\n",
        "        # Prestataire from transporter (since Trackdechet is a platform, not a prestataire)\n",
        "        prestataire = transporter if transporter else GROUPE_PRESTATAIRE\n",
        "\n",
        "        new_row = {\n",
        "            'Libellé': None,\n",
        "            'Groupe': GROUPE,\n",
        "            'Code site': extract_code_site(site_info['nom_site']) if site_info else None,\n",
        "            'Nom du site': site_info['nom_site'] if site_info else 'Site inconnu',\n",
        "            'Nom du client': CLIENT,\n",
        "            'Type de porteur': 'FM',\n",
        "            'Commentaire mouvement': None,\n",
        "            'Code de la prestation': site_info.get('code_prestation') if site_info else None,\n",
        "            'Prestataire': prestataire,\n",
        "            'Groupe de Prestataire': GROUPE_PRESTATAIRE,\n",
        "            'Type de prestataire': TYPE_PRESTATAIRE,\n",
        "            'Périodicité': 'Jour',\n",
        "            'Date début registre': date_val,\n",
        "            'Date fin registre': date_val,\n",
        "            'Code déchet prestataire': waste_code,\n",
        "            'Déchet fin': dechet_fin,\n",
        "            'Déchets agrégé': dechets_agrege,\n",
        "            'Déchets prestataire': dechets_prest if dechets_prest else dechet_fin,\n",
        "            'Masse totale (kg)': masse_kg,\n",
        "            'Nombre de contenants': 1,\n",
        "            'Volume contenant (L)': None,\n",
        "            'Type de contenant': None,\n",
        "            'Volume total (L)': None,\n",
        "            'Nature de quantités collectées': 'Masse',\n",
        "            'Qualité quantités': 'Document prestataire',\n",
        "            'Précision estimations des quantités': None,\n",
        "            'Traitement': traitement,\n",
        "            'Traitement prestataire': None,\n",
        "            'Code traitement': code_final,\n",
        "            'Code traitement prestataire': code_prest,\n",
        "            'Qualité du Traitement': 'Document prestataire',\n",
        "            'N° de BSD/BSDD': bsd,\n",
        "            'N° de recépissé': None,\n",
        "            'Transporteur': transporter,\n",
        "            'Transporteur prestataire': transporter,\n",
        "            \"Plaque d'immatriculation\": None,\n",
        "            'Exutoire intermédiaire': None,\n",
        "            'Exutoire intermédiaire prestataire': None,\n",
        "            \"Qualité de l'exutoire intermédiaire\": None,\n",
        "            'Exutoire final': exutoire,\n",
        "            'Exutoire final prestataire': exutoire,\n",
        "            \"Qualité de l'exutoire final\": 'Document prestataire',\n",
        "            'Période de clôture': None,\n",
        "            'Statut du mouvement': 'Réalisée',\n",
        "            'Commentaire': None,\n",
        "        }\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    df_output = pd.DataFrame(output_rows)\n",
        "    for col in df_template.columns:\n",
        "        if col not in df_output.columns:\n",
        "            df_output[col] = None\n",
        "    df_output = df_output[df_template.columns]\n",
        "\n",
        "    print(f\"✓ Transformed {len(df_output)} rows\")\n",
        "    if skipped_no_weight:\n",
        "        print(f\"  ⚠ Skipped {skipped_no_weight} rows (no weight/incomplete BSD)\")\n",
        "    if skipped_no_site:\n",
        "        print(f\"  ⚠ {skipped_no_site} rows had no site mapping (marked as 'Site inconnu')\")\n",
        "    return df_output\n",
        "\n",
        "def save_output(df_output, output_file):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAVING OUTPUT\")\n",
        "    print(\"=\"*80)\n",
        "    df_output.to_excel(output_file, sheet_name=OUTPUT_SHEET, index=False)\n",
        "    print(f\"✓ Saved: {output_file}\")\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRACKDECHET ETL TRANSFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        mapper = ETLMapper(ETL_FILE, PRESTATAIRE_PATTERN)\n",
        "        df_input, cols = read_input(INPUT_FILE)\n",
        "        df_template = read_template(TEMPLATE_FILE)\n",
        "        df_output = transform(df_input, df_template, mapper, cols)\n",
        "        save_output(df_output, OUTPUT_FILE)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✓✓✓ SUCCESS ✓✓✓\")\n",
        "        print(\"=\"*80)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8zNWwTKNRhx",
        "outputId": "d9c677e8-a2de-4404-a0b7-98e1f8a35080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TRACKDECHET ETL TRANSFORMATION\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING ETL MAPPINGS FOR: Trackdechet\n",
            "================================================================================\n",
            "✓ Déchet: 0 mappings\n",
            "✓ Paramètres: 121 mappings\n",
            "✓ Traitement: 169 mappings\n",
            "✓ Site: 0 mappings\n",
            "\n",
            "================================================================================\n",
            "READING INPUT FILE\n",
            "================================================================================\n",
            "✓ Loaded: 9 rows, 142 columns\n",
            "\n",
            "  Detected columns:\n",
            "    bsd: N° de bordereau\n",
            "    date: Date de réalisation de l'opération\n",
            "    waste_name: Dénomination usuelle\n",
            "    waste_code: Code du déchet\n",
            "    weight: Quantité réceptionnée nette (tonnes)\n",
            "    site_name: Expéditeur Nom usuel\n",
            "    site_address: Expéditeur Adresse\n",
            "    site_raison_sociale: Expéditeur raison sociale\n",
            "    treatment_code_planned: Code opération prévu\n",
            "    treatment_code_done: Code opération réalisé\n",
            "    transporter: Transporteur raison sociale\n",
            "    destination: Destination raison sociale\n",
            "    type_bordereau: Type de bordereau\n",
            "    dangerous: Déchet dangereux\n",
            "\n",
            "================================================================================\n",
            "READING TEMPLATE\n",
            "================================================================================\n",
            "✓ Template: 64 columns\n",
            "\n",
            "================================================================================\n",
            "TRANSFORMING DATA\n",
            "================================================================================\n",
            "  ⚠ No site mapping for: '147'\n",
            "  ⚠ No site mapping for: 'LYON IVOIRE - FR178'\n",
            "  ⚠ No site mapping for: 'LYON IVOIRE - FR178'\n",
            "  ⚠ No site mapping for: 'LYON IVOIRE - FR178'\n",
            "  ⚠ No site mapping for: 'LYON IVOIRE - FR178'\n",
            "  ⚠ No site mapping for: 'LYON IVOIRE - FR178'\n",
            "  ⚠ No site mapping for: 'LYON IVOIRE - FR178'\n",
            "✓ Transformed 7 rows\n",
            "  ⚠ Skipped 2 rows (no weight/incomplete BSD)\n",
            "  ⚠ 7 rows had no site mapping (marked as 'Site inconnu')\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "✓ Saved: /content/Trackdechet_Registre_Agrege_OUTPUT.xlsx\n",
            "\n",
            "================================================================================\n",
            "✓✓✓ SUCCESS ✓✓✓\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}